{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Utiliser NLPModels pour l'optimisation sans contraintes\n",
    "Tangi Migot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial, we introduce NLPModels for numerical optimization. In particular, we will see how to create a structure providing the derivatives of an optimization problem, and then how we can use Ipopt to solve it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this short tutorial, we will use three new packages that belongs to the organization [JuliaSmoothOptimizers](https://juliasmoothoptimizers.github.io/) developed at Polytechnique Montréal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg; \n",
    "Pkg.activate(\"nlpmodels\") # use the closed environment defined in the nlpmodels folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels #Pkg.add(\"ADNLPModels\")\n",
    "using NLPModels #Pkg.add(\"NLPModels\")\n",
    "using NLPModelsJuMP #Pkg.add(\"NLPModelsJuMP\")\n",
    "using NLPModelsIpopt #Pkg.add(\"NLPModelsIpopt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There exists an online documentation for them:\n",
    "- NLPModels: [https://juliasmoothoptimizers.github.io/NLPModels.jl/latest/tutorial/](https://juliasmoothoptimizers.github.io/NLPModels.jl/latest/tutorial/)\n",
    "- NLPModelsJuMP: [https://juliasmoothoptimizers.github.io/NLPModelsJuMP.jl/dev/tutorial/](https://juliasmoothoptimizers.github.io/NLPModelsJuMP.jl/dev/tutorial/)\n",
    "- NLPModelsIpopt: [https://juliasmoothoptimizers.github.io/NLPModelsIpopt.jl/stable/tutorial/#Tutorial-1](https://juliasmoothoptimizers.github.io/NLPModelsIpopt.jl/stable/tutorial/#Tutorial-1)\n",
    "- ADNLPModels: [https://juliasmoothoptimizers.github.io/ADNLPModels.jl/latest/tutorial/](https://juliasmoothoptimizers.github.io/ADNLPModels.jl/latest/tutorial/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLPModels\n",
    "\n",
    "What is NLPModels? An NLPModel is a structure allowing us to easily access the derivatives of an optimization problem. There are two principle ways to create such a structure:\n",
    "i)  Provide the objective function to ADNLPModel, and uses Julia's automatic differentiation;\n",
    "ii) Convert a problem created in JuMP to an NLPModel.\n",
    "\n",
    "### 1) Using automatic differentiation\n",
    "\n",
    "For this purpose, NLPModels has a structure called *ADNLPModel* which is used for unconstrained optimization as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels #call the package ADNLPModels\n",
    "\n",
    "#Initialize the objective function, and an initial guess\n",
    "f(x) = (x[1] - 1)^2 + 100*(x[2] - x[1]^2)^2\n",
    "x0 = [-1.2; 1.0]\n",
    "\n",
    "#Create an ADNLPModel\n",
    "nlp = ADNLPModel(f, x0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the print, we can already see a number of important information:\n",
    "- *ADNLPModel - Model with automatic differentiation* as planned Julia uses the automatic differentiation.\n",
    "- There are a number information relative to the size of the problem. These information are stored in the **meta**.\n",
    "- There are **Counters**, which are keeping track of the number of evaluations of the objective function, gradient, hessian ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.meta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp.meta.nvar #returns the size of x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(nlp.counters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neval_obj(nlp) #get the number of evaluation of the objective function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the objective function and its derivatives, we have at hand the following functions:\n",
    "- `obj`\n",
    "- `grad`\n",
    "- `hess`\n",
    "- `hprod`\n",
    "- and more..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(2)\n",
    "obj(nlp, x)\n",
    "grad(nlp, x)\n",
    "v = rand(2) #vector of size 2 with random numbers between 0 and 1\n",
    "hprod(nlp, x, v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is very important to note here that the function `hess` returns only the lower triangular of the hessian matrix, since it is always a symmetric matrix and it uses less memory this way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hess(nlp, x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hprod(nlp, x, v) - Symmetric(hess(nlp, x), :L) * v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Use a JuMP model\n",
    "\n",
    "For this purpose, we use a package associated to `NLPModels`, which is called `NLPModelsJuMP`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels, NLPModelsJuMP, JuMP\n",
    "\n",
    "x0 = [-1.2; 1.0]\n",
    "model = Model() # No solver is required\n",
    "@variable(model, x[i=1:2], start=x0[i])\n",
    "@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n",
    "\n",
    "nlp_jump = MathOptNLPModel(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To access the objective function and the derivatives, we proceed the exact same way as for `ADNLPModel` before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = ones(2)\n",
    "obj(nlp_jump, x)\n",
    "grad(nlp_jump, x)\n",
    "v = rand(2)\n",
    "hprod(nlp_jump, x, v)\n",
    "hess(nlp_jump, x) #sparse matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solve an NLPModel with Ipopt\n",
    "\n",
    "The problem created with `JuMP` can be solved with `Ipopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using JuMP, Ipopt\n",
    "\n",
    "model = Model(Ipopt.Optimizer)\n",
    "x0 = [-1.2; 1.0]\n",
    "@variable(model, x[i=1:2], start=x0[i])\n",
    "@NLobjective(model, Min, (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2)\n",
    "optimize!(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same can be achieved very easily with `NLPModels` using the package `NLPModelsIpopt` and its function `ipopt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using NLPModels, NLPModelsIpopt\n",
    "\n",
    "nlp = ADNLPModel(x -> (x[1] - 1)^2 + 100 * (x[2] - x[1]^2)^2, [-1.2; 1.0])\n",
    "stats = ipopt(nlp)\n",
    "print(stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
