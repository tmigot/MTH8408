{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 2: Optimisation sans contraintes\n",
    "Tangi Migot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `C:\\Users\\Jean-Luc\\.julia\\registries\\General.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Manifest.toml`\n",
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"ADNLPModels\")\n",
    "Pkg.add(\"NLPModels\")\n",
    "using ADNLPModels, LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra trouver de la documentation sur `ADNLPModels` et `NLPModels` ici:\n",
    "- [juliasmoothoptimizers.github.io/NLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/NLPModels.jl/dev/)\n",
    "- [juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Newton avec recherche linéaire - amélioration du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, vous avez le code de deux fonctions qui ont été vues dans les capsules, la recherche linéaire qui satisfait Armijo, et une méthode de Newton avec cette recherche linéaire. Le but de ce laboratoire est d'implémenter d'autres méthodes utiles pour résoudre des problèmes de grandes dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Amélioration possibles: return also the value of f\n",
    "function armijo(xk, dk, fk, gk, f)\n",
    "  slope = dot(gk, dk) #doit être <0\n",
    "  t = 1.0\n",
    "  while f(xk + t * dk) > fk + 1.0e-4 * t * slope\n",
    "    t /= 1.5\n",
    "  end\n",
    "  return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[32m\u001b[1mTest Passed\u001b[22m\u001b[39m\n",
       "  Expression: f(xk + t * dk) <= f(xk) + 0.0001 * t * dot(g(xk), dk)\n",
       "   Evaluated: -0.37311385459533614 <= -0.00017777777777777779"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test pour vérifier que la fonction armijo fonctionne correctement.\n",
    "using Test #le package Test définit (entre autre) la macro @test qui permet de faire des tests unitaires :-)\n",
    "xk = ones(2)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "fk = f(xk)\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= fk + 1.0e-4 * t * dot(gk,dk)\n",
    "\n",
    "xk = [1.5, 0.5]\n",
    "fk = f(xk)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= f(xk) + 1.0e-4 * t * dot(g(xk),dk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton_armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "function newton_armijo(f, g, H, x0)\n",
    "  xk  = x0\n",
    "  fk  = f(xk)\n",
    "  gk = g(xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100\n",
    "    Hk = H(xk)\n",
    "    dk = - Hk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "    t = armijo(xk, dk, fk, gk, f)\n",
    "    xk += t * dk\n",
    "    fk = f(xk)\n",
    "    gk = g(xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problème test:\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1) # fonction objectif vue en classe\n",
    "g(x) = 6 * [x[1]^2 - x[1] - 2*x[1]*x[2] + x[2]^2 + x[2]; -x[1]^2 + 2*x[1]*x[2] + x[1]] # le gradient de f\n",
    "H(x) = 6 * [2*x[1]-1-2*x[2] -2*x[1]+2*x[2]+1; -2*x[1]+2*x[2]+1 2*x[1]] # la Hessienne de f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||\n",
      " 0  1.00e+00   4.5e+00\n",
      " 1  5.08e-02   8.4e-01 1.0e+00 \n",
      " 2  4.73e-04   7.6e-02 1.0e+00 \n",
      " 3  6.97e-08   9.1e-04 1.0e+00 \n",
      " 4  1.62e-15   1.4e-07 1.0e+00 \n",
      " k        fk ||∇f(x)||\n",
      " 0  1.00e+00   4.5e+00\n",
      " 1  5.08e-02   8.4e-01 1.0e+00 \n",
      " 2  4.73e-04   7.6e-02 1.0e+00 \n",
      " 3  6.97e-08   9.1e-04 1.0e+00 \n",
      " 4  1.62e-15   1.4e-07 1.0e+00 \n",
      "[2.3230573665069826e-8, 2.3230573665069826e-8]"
     ]
    }
   ],
   "source": [
    "sol = newton_armijo(f, g, H, [.5, .5])\n",
    "@test g(sol) ≈ zeros(2) atol = 1.0e-6\n",
    "@printf \"%s\" sol"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut améliorer le code de la fonction `newton_armijo` avec les ajouts suivants:\n",
    "- Changer les paramètre d'entrées de la fonction pour un `nlp`\n",
    "- Avant d'appeler la recherche linéaire, si `slope = dot(dk, gk)` est plus grand que `-1.0e-4 * norm(dk) * gnorm`, on modifie le système. On fait maximum 5 mise à jour de `λ`, sinon on prend l'opposé du gradient.\n",
    "```\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "```\n",
    "Ajouter un compteur sur le nombre de mises à jour de `λ` et ajuster `dk = - gk` si la limite est atteinte.\n",
    "- On veut aussi détecter et éventuellement arrêter la boucle `while` si la fonction objectif `fk` devient trop petite/négative, i.e. le problème est non-bornée inférieurement.\n",
    "- On veut ajouter deux critères d'arrêts supplémentaires: \n",
    "  - un compteur sur le nombre d'évaluations de f (maximum 1000). Utiliser `neval_obj(nlp)`.\n",
    "  -  une limite de temps d'execution. Utiliser la fonction `time()`.\n",
    "- Enfin, on voudrait aussi voir un message à l'écran si l'algorithme n'a pas trouvé la solution, i.e. il s'est arrêté à cause de la limite sur le nombre d'itérations, temps, évaluation de fonctions, problème non-borné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||\n",
      " 0  1.00e+00   4.5e+00\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "newton_armijo (generic function with 2 methods)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SOLUTION: fonction à modifier\n",
    "function newton_armijo(nlp, x0)\n",
    "  xk  = x0\n",
    "  fk  = obj(nlp, xk)\n",
    "  gk = grad(nlp, xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  start = time()\n",
    "  @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100 && neval_obj(nlp) < 1000 && time()-start <= 60.0 && abs(obj(nlp, xk)) <= 1.0e8\n",
    "    Hk = hess(nlp, xk)\n",
    "    dk = - Hk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    λ = 0.0\n",
    "    lcounter = 1\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm && lcounter <= 7\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "      lcounter += 1\n",
    "    end\n",
    "    if lcounter > 7\n",
    "      dk = - gk\n",
    "      @printf \"%d Iterations of lambda reached\" \"lcounter\"\n",
    "    end\n",
    "    t = armijo(xk, dk, fk, gk, f)\n",
    "    xk += t * dk\n",
    "    fk  = obj(nlp, xk)\n",
    "    gk = grad(nlp, xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||\n",
      " 0  1.00e+00   4.5e+00\n",
      " 1  5.08e-02   8.4e-01 1.0e+00 \n",
      " 2  4.73e-04   7.6e-02 1.0e+00 \n",
      " 3  6.97e-08   9.1e-04 1.0e+00 \n",
      " 4  1.62e-15   1.4e-07 1.0e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.323057366509693e-8\n",
       " 2.323057366509693e-8"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "x0 =  [.5, .5]\n",
    "nlp = ADNLPModel(f, x0)\n",
    "\n",
    "newton_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests secrets 1: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: LDLt-Newton avec recherche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant modifier la méthode de Newton vu précédemment pour utiliser un package qui s'occupe de calculer une factorisation de la matrice hessienne tel que:\n",
    "$$\n",
    "\\nabla^2 f(x) = LDL^T.\n",
    "$$\n",
    "Ce type de factorisation n'est possible que si la matrice hessienne est définie positive, dans le cas contraire on a besoin de régularisé le système comme dans l'exercice précédent.\n",
    "\n",
    "Pour résoudre le système linéaire en utilisant cette factorisation, on va utiliser le package [`LDLFactorizations`](https://github.com/JuliaSmoothOptimizers/LDLFactorizations.jl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `C:\\Users\\Jean-Luc\\.julia\\environments\\v1.7\\Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"LDLFactorizations\")\n",
    "using LDLFactorizations, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tutoriel sur l'utilisation de `LDLFactorizations` est disponible sur la documentation du package sur github ou encore [à ce lien](https://juliasmoothoptimizers.github.io/LDLFactorizations.jl/dev/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple d'utilisation de ce package. La matrice dont on veut calculer la factorisation doit être de type `Symmetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "true"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = ones(2,2) #cette matrice symétrique, mais pas du type Symmetric\n",
    "              #à noter que cette matrice n'est pas définie positive.\n",
    "typeof(A) <: Symmetric #false\n",
    "A = Symmetric(A)\n",
    "typeof(A) <: Symmetric #true :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième étape, le package fait une phase d'analyse de la matrice avec `ldl_analyze` en créant une structure pratique pour les diverses fonctions du package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = -rand(2, 2)\n",
    "sol = rand(2)\n",
    "b = A*sol #on veut résoudre le système A*x=b\n",
    "\n",
    "# LDLFactorizations va en réalité demander la matrice triangulaire supérieure\n",
    "A = Symmetric(triu(A), :U)\n",
    "\n",
    "S = ldl_analyze(A)\n",
    "\n",
    "ldl_factorize!(A, S) \n",
    "x = S \\ b # x = A \\b ça va être résolu par Julia \n",
    "norm(A * x - b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice `A` factorisée par $LDL^T$ n'était pas forcément définie positive. On peut le voir sur les valeurs de $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " -0.3619689267912928\n",
       "  0.11895679244576385"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S.d #c'est le vecteur qui correspond à la matrice diagonale D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'optimisation, dans le cas où des valeurs de $D$ sont négatives, i.e. `minimum(S.d) <= 0.`, on ajoutera une correction pour être sûr d'obtenir une direction de descente. On pourra choisir un des deux:\n",
    "- `S.d   = abs.(S.d)` #la valeur absolue des valeurs négatives\n",
    "- `S.d .+= minimum(S.d) + epsilon`\n",
    "\n",
    "`LDLFactorizations` a une façon automatique d'utiliser la deuxième option, il suffit de modifier la structure `S` obtenue par `ldl_analyze` avant la factorisation:\n",
    "```\n",
    "S.tol = 1e-8 #régularise les valeurs plus petites que cette tolérance\n",
    "S.n_d = 0 #taille de la régularisation\n",
    "S.r1 = -eps()^(1/4)\n",
    "S.r2 = eps()^(1/2)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser cette technique pour calculer la direction de descente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton_ldlt_armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution: copier-coller votre newton_armijo ici et modifier le calcul de la direction avec LDLFactorizations\n",
    "function newton_ldlt_armijo(nlp, x0)\n",
    "  xk  = x0\n",
    "  fk  = obj(nlp, xk)\n",
    "  gk = grad(nlp, xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  start = time()\n",
    "  @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100 && neval_obj(nlp) < 1000 && time()-start <= 60.0 && abs(obj(nlp, xk)) <= 1.0e8\n",
    "    Hk = hess(nlp, xk)\n",
    "    \n",
    "    Hk = Symmetric(triu(Hk), :U)\n",
    "    S = ldl_analyze(Hk)\n",
    "    S.tol = 1e-8 #régularise les valeurs plus petites que cette tolérance\n",
    "    S.n_d = 0 #taille de la régularisation\n",
    "    S.r1 = -eps()^(1/4)\n",
    "    S.r2 = eps()^(1/2)\n",
    "    ldl_factorize!(Hk, S)\n",
    "    dk = -S \\ gk\n",
    "\n",
    "    t = armijo(xk, dk, fk, gk, f)\n",
    "    xk += t * dk\n",
    "    fk  = obj(nlp, xk)\n",
    "    gk = grad(nlp, xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "UndefVarError",
     "evalue": "UndefVarError: ldl_analyse not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: ldl_analyse not defined\n",
      "\n",
      "Stacktrace:\n",
      "  [1] newton_ldlt_armijo(nlp::ADNLPModel{Float64, Vector{Float64}}, x0::Vector{Float64})\n",
      "    @ Main c:\\Users\\Jean-Luc\\Documents\\PolyMtl\\Git\\MTH8408\\lab2\\Lab2-notebook.ipynb:15\n",
      "  [2] top-level scope\n",
      "    @ c:\\Users\\Jean-Luc\\Documents\\PolyMtl\\Git\\MTH8408\\lab2\\Lab2-notebook.ipynb:5\n",
      "  [3] eval\n",
      "    @ .\\boot.jl:373 [inlined]\n",
      "  [4] include_string(mapexpr::typeof(REPL.softscope), mod::Module, code::String, filename::String)\n",
      "    @ Base .\\loading.jl:1196\n",
      "  [5] #invokelatest#2\n",
      "    @ .\\essentials.jl:716 [inlined]\n",
      "  [6] invokelatest\n",
      "    @ .\\essentials.jl:714 [inlined]\n",
      "  [7] (::VSCodeServer.var\"#150#151\"{VSCodeServer.NotebookRunCellArguments, String})()\n",
      "    @ VSCodeServer c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:18\n",
      "  [8] withpath(f::VSCodeServer.var\"#150#151\"{VSCodeServer.NotebookRunCellArguments, String}, path::String)\n",
      "    @ VSCodeServer c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\packages\\VSCodeServer\\src\\repl.jl:185\n",
      "  [9] notebook_runcell_request(conn::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, params::VSCodeServer.NotebookRunCellArguments)\n",
      "    @ VSCodeServer c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:14\n",
      " [10] dispatch_msg(x::VSCodeServer.JSONRPC.JSONRPCEndpoint{Base.PipeEndpoint, Base.PipeEndpoint}, dispatcher::VSCodeServer.JSONRPC.MsgDispatcher, msg::Dict{String, Any})\n",
      "    @ VSCodeServer.JSONRPC c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\packages\\JSONRPC\\src\\typed.jl:67\n",
      " [11] serve_notebook(pipename::String; crashreporting_pipename::String)\n",
      "    @ VSCodeServer c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\packages\\VSCodeServer\\src\\serve_notebook.jl:94\n",
      " [12] top-level scope\n",
      "    @ c:\\Users\\Jean-Luc\\.vscode\\extensions\\julialang.language-julia-1.5.10\\scripts\\notebook\\notebook.jl:12\n",
      " [13] include(mod::Module, _path::String)\n",
      "    @ Base .\\Base.jl:418\n",
      " [14] exec_options(opts::Base.JLOptions)\n",
      "    @ Base .\\client.jl:292\n",
      " [15] _start()\n",
      "    @ Base .\\client.jl:495"
     ]
    }
   ],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, [.5, .5])\n",
    "\n",
    "newton_ldlt_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests secrets 2: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Méthode quasi-Newton: BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode quasi-Newton: BFGS\n",
    "Pour des problèmes de très grandes tailles, il est parfois très coûteux d'évaluer la hessienne du problème d'optimisation (et même le produit hessienne-vecteur). La famille des méthode *quasi-Newton* construit une approximation $B_k$ symétrique définie positive de la matrice Hessienne en utilisant seulement le gradient et en mesurant sa variation, et permet quand même d'améliorer significativement les performances comparé à la méthode du gradient.\n",
    "$$\n",
    "s_k = x_{k+1} - x_k, \\quad y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k).\n",
    "$$\n",
    "Par ailleurs la matrice $B_k$ est aussi construite de façon à ce que l'inverse soit connue, il n'y a donc pas de système linéaire à résoudre.\n",
    "\n",
    "La méthode la plus connue dans la famille des méthodes quasi-Newton, est la méthode BFGS (Broyden - Fletcher, Goldfarb, and Shanno), et utilise la formule suivante pour calculer l'inverse de $B_k$ que l'on note $H_k$:\n",
    "$$\n",
    "H_{k+1} = (I - \\rho_k s_ky_k^T)H_k(I-\\rho_ky_ks_k^T) + \\rho_ks_ks_k^T, \\quad \\rho_k = \\frac{1}{y_k^Ts_k}.\n",
    "$$\n",
    "L'algorithme est presque le même que la méthode de Newton à la différence qu'il n'y a pas de système linéaire à résoudre et la direction $d_k$ est à coup sûr une direction de descente. Ainsi la direction de descente est calculée comme suit:\n",
    "$$\n",
    "d_k = - H_k \\nabla f(x_k).\n",
    "$$\n",
    "\n",
    "Comment choisir la matrice $H_0$? On peut éventuellement choisir $I$. Une alternative est d'utiliser $H_0=I$ pour la première itération et ensuite mettre $H_0$ à jour avant de calculer $H_1$ en utilisant:\n",
    "$$\n",
    "H_0 = \\frac{y_k^Ts_k}{y_k^Ty_k}I.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: pour s'assurer que la matrice $H_k$ reste définie positive à toutes les itérations, il faut s'assurer que $y_k^Ts_k>0$. C'est toujours vrai pour des fonctions convexes, mais pas nécessairement dans le cas général. On pourra tester ici la version \"skip\" qui ne mets pas à jour quand cette condition n'est pas vérifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solution: copier-coller votre newton_armijo ici et modifier le calcul de la direction avec la méthode de BFGS inverse skip.\n",
    "function bfgs_quasi_newton_armijo(nlp, x0)\n",
    "  xk = x0\n",
    "  #à compléter\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, zeros(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tests secrets 3: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: application à un problème de grande taille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ajouter le package `OptimizationProblems` qui contient, comme son nom l'indique, une collection de problème d'optimisation disponible au format de `JuMP` (dans le sous-module `OptimizationProblems.PureJuMP`) et de `ADNLPModel` (dans le sous-module `OptimizationProblems.ADNLPProblems`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels, OptimizationProblems.ADNLPProblems # Attention si vous ne faites pas using ADNLPModels avant ça ne fonctionne pas!\n",
    "\n",
    "n = 500\n",
    "model = genrose(n=n)\n",
    "@test typeof(model) == ADNLPModel{Float64, Vector{Float64}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous le souhaitez, il est possible d'accéder à certaines informations sur le problème en accédant à son meta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems\n",
    "OptimizationProblems.genrose_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résoudre le problème `genrose` et un autre problème de la collection en utilisant vos algorithmes précédents.\n",
    "Avant d'utiliser l'algorithme on testera que le problème est bien sans contrainte avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained(nlp) #qui retourne vrai si `nlp` est un problème sans contraintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.7.1",
   "language": "julia",
   "name": "julia-1.7"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
