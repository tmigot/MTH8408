{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 2: Optimisation sans contraintes\n",
    "Tangi Migot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Project.toml`\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/.julia/environments/v1.8/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"NLPModels\")\n",
    "using ADNLPModels, LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On pourra trouver de la documentation sur `ADNLPModels` et `NLPModels` ici:\n",
    "- [juliasmoothoptimizers.github.io/NLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/NLPModels.jl/dev/)\n",
    "- [juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/](https://juliasmoothoptimizers.github.io/ADNLPModels.jl/dev/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "H (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Problème test:\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1) # fonction objectif vue en classe\n",
    "g(x) = 6 * [x[1]^2 - x[1] - 2*x[1]*x[2] + x[2]^2 + x[2]; -x[1]^2 + 2*x[1]*x[2] + x[1]] # le gradient de f\n",
    "H(x) = 6 * [2*x[1]-1-2*x[2] -2*x[1]+2*x[2]+1; -2*x[1]+2*x[2]+1 2*x[1]] # la Hessienne de f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 1: Newton avec recherche linéaire - amélioration du code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ci-dessous, vous avez le code de deux fonctions qui ont été vues dans le cours, la recherche linéaire qui satisfait Armijo, et une méthode de Newton avec cette recherche linéaire. Le but de ce laboratoire est d'implémenter d'autres méthodes utiles pour résoudre des problèmes de grandes dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Amélioration possibles: return also the value of f\n",
    "function armijo(xk, dk, fk, gk, f)\n",
    "  slope = dot(gk, dk) #doit être <0\n",
    "  t = 1.0\n",
    "  while f(xk + t * dk) > fk + 1.0e-4 * t * slope\n",
    "    t /= 1.5\n",
    "  end\n",
    "  return t\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Test pour vérifier que la fonction armijo fonctionne correctement.\n",
    "using Test #le package Test définit (entre autre) la macro @test qui permet de faire des tests unitaires :-)\n",
    "xk = ones(2)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "fk = f(xk)\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= fk + 1.0e-4 * t * dot(gk,dk)\n",
    "\n",
    "xk = [1.5, 0.5]\n",
    "fk = f(xk)\n",
    "gk = g(xk)\n",
    "dk = - gk\n",
    "t  = armijo(xk, dk, fk, gk, f)\n",
    "@test t < 1\n",
    "@test f(xk + t * dk) <= f(xk) + 1.0e-4 * t * dot(g(xk),dk)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function newton_armijo(f, g, H, x0; verbose::Bool = true)\n",
    "  xk  = x0\n",
    "  fk  = f(xk)\n",
    "  gk = g(xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  verbose && @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100\n",
    "    Hk = H(xk)\n",
    "    dk = - Hk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "    t = armijo(xk, dk, fk, gk, f)\n",
    "    xk += t * dk\n",
    "    fk = f(xk)\n",
    "    gk = g(xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sol = newton_armijo(f, g, H, [.5, .5])\n",
    "@test g(sol) ≈ zeros(2) atol = 1.0e-6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On veut améliorer le code de la fonction `newton_armijo` avec les ajouts suivants:\n",
    "- Changer les paramètre d'entrées de la fonction pour un `nlp`\n",
    "- Avant d'appeler la recherche linéaire, si `slope = dot(dk, gk)` est plus grand que `-1.0e-4 * norm(dk) * gnorm`, on modifie le système. On fait maximum 5 mise à jour de `λ`, sinon on prend l'opposé du gradient.\n",
    "```\n",
    "    λ = 0.0\n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      λ = max(1.0e-3, 10 * λ)\n",
    "      dk = - ((Hk + λ * I ) \\ gk)\n",
    "      slope = dot(dk, gk)\n",
    "    end\n",
    "```\n",
    "Ajouter un compteur sur le nombre de mises à jour de `λ` et ajuster `dk = - gk` si la limite est atteinte.\n",
    "- On veut aussi détecter et éventuellement arrêter la boucle `while` si la fonction objectif `fk` devient trop petite/négative (inférieure à `-1e15`), i.e. le problème est non-bornée inférieurement.\n",
    "- On veut ajouter deux critères d'arrêts supplémentaires: \n",
    "  - un compteur sur le nombre d'évaluations de f (maximum 1000). Utiliser `neval_obj(nlp)`.\n",
    "  -  une limite de temps d'execution, `max_time = 60.0`. Utiliser la fonction `time()`.\n",
    "- Enfin, on voudrait aussi voir un message à l'écran si l'algorithme n'a pas trouvé la solution, i.e. il s'est arrêté à cause de la limite sur le nombre d'itérations, temps, évaluation de fonctions, problème non-borné."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton_armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#SOLUTION: fonction à modifier\n",
    "function newton_armijo(nlp, x0; verbose::Bool = true)\n",
    "  t1 = time()\n",
    "  xk  = x0\n",
    "  # fk  = f(xk)\n",
    "  fk = obj(nlp,xk)\n",
    "  # gk = g(xk)\n",
    "  gk = grad(nlp,xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  verbose && @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  critereArreteAtteint = [false,false,false,false]\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100\n",
    "    # Hk = H(xk)\n",
    "    Hk = hess(nlp,xk)\n",
    "    dk = - Hk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    λ = 0.0\n",
    "    compteur_λ = 0 \n",
    "    while slope ≥ -1.0e-4 * norm(dk) * gnorm\n",
    "      if compteur_λ < 5 \n",
    "        λ = max(1.0e-3, 10 * λ)\n",
    "        dk = - ((Hk + λ * I ) \\ gk)\n",
    "        slope = dot(dk, gk)\n",
    "        compteur_λ += 1 \n",
    "      else\n",
    "        dk = - gk\n",
    "        critereArreteAtteint[1] = true # limite sur le nombre d'itérations \n",
    "        break  \n",
    "      end\n",
    "    end\n",
    "    t = armijo(xk, dk, fk, gk, x->obj(nlp,x))\n",
    "    xk += t * dk\n",
    "    # fk = f(xk)\n",
    "    fk = obj(nlp,xk)\n",
    "\n",
    "    if neval_obj(nlp) >= 1000 \n",
    "      critereArreteAtteint[3] = # critère d'arrêt sur le nombre d'évaluation de la fonction \n",
    "      break \n",
    "    end \n",
    "\n",
    "    # Arrêter la boucle si fk trop petite\n",
    "    if fk < -1e15 \n",
    "      critereArreteAtteint[4] = true # problème non-borné \n",
    "      break \n",
    "    end\n",
    "    # gk = g(xk)\n",
    "    gk = grad(nlp,xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "    t2 = time()\n",
    "  \n",
    "    if t2 - t1 > 60.0 \n",
    "      critereArreteAtteint[2] = true # critère d'arrêt sur le temps \n",
    "      break \n",
    "    end\n",
    "\n",
    "  end\n",
    "  return xk\n",
    "  if critereArreteAtteint[2]\n",
    "    println(\"L'algorithme n'a pas trouvé de solution, il s'est arrêté à cause de la limite sur le temps d'exécution.\")\n",
    "  elseif critereArreteAtteint[3]\n",
    "    println(\"L'algorithme n'a pas trouvé de solution, il s'est arrêté à cause de la limite sur le nombre d'évaluation de la fonction objectif.\")\n",
    "  elseif critereArreteAtteint[4]\n",
    "    println(\"L'algorithme n'a pas trouvé de solution, il s'est arrêté car le problème est non borné.\")\n",
    "  end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||\n",
      " 0  5.00e+00   1.2e+01\n",
      " 1  4.07e-01   2.7e+00 1.0e+00 \n",
      " 2  1.39e-02   4.3e-01 1.0e+00 \n",
      " 3  4.63e-05   2.4e-02 1.0e+00 \n",
      " 4  6.99e-10   9.2e-05 1.0e+00 \n",
      " 5  1.63e-19   1.4e-09 1.0e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.3283064370807974e-10\n",
       " 2.3283064370807974e-10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "x0 = ones(2)\n",
    "nlp = ADNLPModel(f, x0)\n",
    "\n",
    "newton_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:              | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "Test set for newton_armijo | \u001b[32m   7  \u001b[39m\u001b[36m    7  \u001b[39m\u001b[0m1.9s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Test set for newton_armijo\", Any[], 7, false, false, true, 1.674489895003154e9, 1.674489896861178e9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests secrets 1: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt.\n",
    "using Test\n",
    "\n",
    "@testset \"Test set for newton_armijo\" begin\n",
    "    #Test problem:\n",
    "    fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "    x0H = [10., 20.]\n",
    "    himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "    problem2 = ADNLPModel(x->-x[1]^3 + x[2]^2 + x[3]^2, ones(3))\n",
    "\n",
    "    roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "    rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "    f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "    pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]\n",
    "\n",
    "\n",
    "    ######################################### newton_armijo ##################\n",
    "\n",
    "    #=\n",
    "    Vérifiez que vous pouvez mettre une limite de temps de 1s.\n",
    "    Puis faites tournez votre algorithme sur un des probléme test.\n",
    "    L'algorithme devrait s'arrêter en environ 1sec.\n",
    "\n",
    "    Vérifiez que vous pouvez mettre une limite de 2 évaluations d'objectif.\n",
    "    Puis faites tournez votre algorithme sur un des probléme test.\n",
    "    L'algorithme devrait s'arrêter en environ 1 itération.\n",
    "    =#\n",
    "\n",
    "    #Unit/Validation Tests\n",
    "    ep1 = 1e-6 + norm(grad(himmelblau, himmelblau.meta.x0)) * 1e-6\n",
    "    sol = newton_armijo(himmelblau, himmelblau.meta.x0, verbose = false)\n",
    "    @test norm(grad(himmelblau, sol)) ≤ ep1\n",
    "    @test sol ≈ [3, 2] atol = ep1\n",
    "\n",
    "    ep2 = 1e-6 + norm(grad(problem2, problem2.meta.x0)) * 1e-6\n",
    "    sol =  newton_armijo(problem2, problem2.meta.x0, verbose = false)\n",
    "    # @test stats.status == :unbounded\n",
    "    @test obj(problem2, sol) ≤ -1e15\n",
    "\n",
    "    ep2 = 1e-6 + norm(grad(rosenbrock, rosenbrock.meta.x0)) * 1e-6\n",
    "    sol = newton_armijo(rosenbrock, rosenbrock.meta.x0, verbose = false)\n",
    "    @test sol ≈ [1., 1.] atol = ep2\n",
    "\n",
    "    ep3 = 1e-6 + norm(grad(pb_du_cours, [-1.001, -1.001])) * 1e-6\n",
    "    sol = newton_armijo(pb_du_cours, [-1.001, -1.001], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep3 || obj(pb_du_cours, sol) <= -1e15\n",
    "\n",
    "    ep4 = 1e-6 + norm(grad(pb_du_cours, [1.5, .5])) * 1e-6\n",
    "    sol = newton_armijo(pb_du_cours, [1.5, .5], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep4 || obj(pb_du_cours, sol) <= -1e15\n",
    "\n",
    "    ep5 = 1e-6 + norm(grad(pb_du_cours, [.5, .5])) * 1e-6\n",
    "    sol = newton_armijo(pb_du_cours, [.5, .5], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep5 || obj(pb_du_cours, sol) <= -1e15\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 2: LDLt-Newton avec recherche linéaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va maintenant modifier la méthode de Newton vu précédemment pour utiliser un package qui s'occupe de calculer une factorisation de la matrice hessienne tel que:\n",
    "$$\n",
    "\\nabla^2 f(x) = LDL^T.\n",
    "$$\n",
    "Ce type de factorisation n'est possible que si la matrice hessienne est définie positive, dans le cas contraire on a besoin de régularisé le système comme dans l'exercice précédent.\n",
    "\n",
    "Pour résoudre le système linéaire en utilisant cette factorisation, on va utiliser le package [`LDLFactorizations`](https://github.com/JuliaSmoothOptimizers/LDLFactorizations.jl):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m registry at `~/.julia/registries/General.toml`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m   Resolving\u001b[22m\u001b[39m package versions...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m\u001b[1m    Updating\u001b[22m\u001b[39m `~/Desktop/MTH8408/Project.toml`\n",
      " \u001b[90m [40e66cde] \u001b[39m\u001b[92m+ LDLFactorizations v0.10.0\u001b[39m\n",
      "\u001b[32m\u001b[1m  No Changes\u001b[22m\u001b[39m to `~/Desktop/MTH8408/Manifest.toml`\n"
     ]
    }
   ],
   "source": [
    "import Pkg; Pkg.add(\"LDLFactorizations\")\n",
    "using LDLFactorizations, LinearAlgebra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un tutoriel sur l'utilisation de `LDLFactorizations` est disponible sur la documentation du package sur github ou encore [à ce lien](https://juliasmoothoptimizers.github.io/LDLFactorizations.jl/dev/tutorial/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici un exemple d'utilisation de ce package. La matrice dont on veut calculer la factorisation doit être de type `Symmetric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = ones(2,2) #cette matrice symétrique, mais pas du type Symmetric\n",
    "              #à noter que cette matrice n'est pas définie positive.\n",
    "typeof(A) <: Symmetric #false\n",
    "A = Symmetric(A)\n",
    "typeof(A) <: Symmetric #true :)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deuxième étape, le package fait une phase d'analyse de la matrice avec `ldl_analyze` en créant une structure pratique pour les diverses fonctions du package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4.965068306494546e-16"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = -rand(2, 2)\n",
    "sol = rand(2)\n",
    "b = A*sol #on veut résoudre le système A*x=b\n",
    "\n",
    "# LDLFactorizations va en réalité demander la matrice triangulaire supérieure\n",
    "A = Symmetric(triu(A), :U)\n",
    "S = ldl_analyze(A)\n",
    "ldl_factorize!(A, S)\n",
    "x = S \\ b # x = A \\b ça va être résolu par Julia \n",
    "norm(A * x - b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2×2 Matrix{Float64}:\n",
       " 0.0  1.0\n",
       " 1.0  0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = [0. 1.; 1. 0.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LDLFactorizations.LDLFactorization{Float64, Int64, Int64, Int64}(true, false, true, 2, [2, -1], [0, 0], [1, 2], [1, 2], [1, 2], [1, 2, 2], [1, 1, 1], Int64[], [4764090000], [3.0453972104e-314], [0.0, 2.354564522e-314], [0.0, 5.0e-324], [4766933904, 4766933920], 0.0, 0.0, 0.0, 2)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "A = Symmetric(triu(A), :U)\n",
    "S = ldl_analyze(A)\n",
    "ldl_factorize!(A, S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.0\n",
       " 2.354564522e-314"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S.d "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La matrice `A` factorisée par $LDL^T$ n'était pas forcément définie positive. On peut le voir sur les valeurs de $D$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "false"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "S.d #c'est le vecteur qui correspond à la matrice diagonale D."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour l'optimisation, dans le cas où des valeurs de $D$ sont négatives, i.e. `minimum(S.d) <= 0.`, on ajoutera une correction pour être sûr d'obtenir une direction de descente. On pourra choisir un des deux:\n",
    "- `S.d   = abs.(S.d)`\n",
    "- `S.d .+= -minimum(S.d) + 1e-6`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Utiliser cette technique pour calculer la direction de descente:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "newton_ldlt_armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution: modifier le calcul de la direction avec LDLFactorizations\n",
    "function newton_ldlt_armijo(nlp, x0; verbose::Bool = true)\n",
    "  xk  = x0\n",
    "  fk  = obj(nlp, xk)\n",
    "  gk = grad(nlp, xk)\n",
    "  gnorm = gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  verbose && @printf \"%2s %9s %9s\\n\" \"k\" \"fk\" \"||∇f(x)||\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e\\n\" k fk gnorm\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100 && fk > -1e15\n",
    "    Hk = Symmetric(triu(hess(nlp, xk)), :U)\n",
    "    # ... TODO ...\n",
    "    Sk = ldl_analyze(Hk)\n",
    "    ldl_factorize!(Hk, Sk)\n",
    "    if !all(Sk.d .>= 0) \n",
    "      Sk.d .+= -minimum(Sk.d) + 1e-6\n",
    "    end\n",
    "    dk = - Sk \\ gk\n",
    "    slope = dot(dk, gk)\n",
    "    t = armijo(xk, dk, fk, gk, x -> obj(nlp, x))\n",
    "    xk += t * dk\n",
    "    fk = obj(nlp, xk)\n",
    "    gk = grad(nlp, xk)\n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||\n",
      " 0  5.00e+00   1.2e+01\n",
      " 1  4.07e-01   2.7e+00 1.0e+00 \n",
      " 2  1.39e-02   4.3e-01 1.0e+00 \n",
      " 3  4.63e-05   2.4e-02 1.0e+00 \n",
      " 4  6.99e-10   9.2e-05 1.0e+00 \n",
      " 5  1.63e-19   1.4e-09 1.0e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 2.3283064370807974e-10\n",
       " 2.3283064370807974e-10"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, zeros(2))\n",
    "\n",
    "newton_ldlt_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[1mTest Summary:                   | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "Test set for newton_ldlt_armijo | \u001b[32m   7  \u001b[39m\u001b[36m    7  \u001b[39m\u001b[0m1.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Test set for newton_ldlt_armijo\", Any[], 7, false, false, true, 1.674491773364915e9, 1.674491774865108e9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests secrets 2: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt.\n",
    "\n",
    "using Test\n",
    "\n",
    "@testset \"Test set for newton_ldlt_armijo\" begin\n",
    "    #Test problem:\n",
    "    fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "    x0H = [10., 20.]\n",
    "    himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "    problem2 = ADNLPModel(x->-x[1]^3 + x[2]^2 + x[3]^2, ones(3))\n",
    "\n",
    "    roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "    rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "    f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "    pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]\n",
    "\n",
    "    #Unit/Validation Tests\n",
    "    ep1 = 1e-6 + norm(grad(himmelblau, himmelblau.meta.x0)) * 1e-6\n",
    "    sol = newton_ldlt_armijo(himmelblau, himmelblau.meta.x0, verbose = false)\n",
    "    @test norm(grad(himmelblau, sol)) ≤ ep1\n",
    "    @test sol ≈ [3, 2] atol = ep1\n",
    "\n",
    "    ep2 = 1e-6 + norm(grad(problem2, problem2.meta.x0)) * 1e-6\n",
    "    sol = newton_ldlt_armijo(problem2, problem2.meta.x0, verbose = false)\n",
    "    # @test stats.status == :unbounded\n",
    "    @test obj(problem2, sol) ≤ -1e15\n",
    "\n",
    "    ep2 = 1e-6 + norm(grad(rosenbrock, rosenbrock.meta.x0)) * 1e-6\n",
    "    sol = newton_ldlt_armijo(rosenbrock, rosenbrock.meta.x0, verbose = false)\n",
    "    @test sol ≈ [1., 1.] atol = ep2\n",
    "\n",
    "    ep3 = 1e-6 + norm(grad(pb_du_cours, [-1.001, -1.001])) * 1e-6\n",
    "    sol = newton_ldlt_armijo(pb_du_cours, [-1.001, -1.001], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep3 || obj(pb_du_cours, sol) <= -1e15\n",
    "\n",
    "    ep4 = 1e-6 + norm(grad(pb_du_cours, [1.5, .5])) * 1e-6\n",
    "    sol = newton_ldlt_armijo(pb_du_cours, [1.5, .5], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep4 || obj(pb_du_cours, sol) <= -1e15\n",
    "\n",
    "    ep5 = 1e-6 + norm(grad(pb_du_cours, [.5, .5])) * 1e-6\n",
    "    sol = newton_ldlt_armijo(pb_du_cours, [.5, .5], verbose = false)\n",
    "    @test norm(grad(pb_du_cours, sol)) ≤ ep5 || obj(pb_du_cours, sol) <= -1e15\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 3: Méthode quasi-Newton: BFGS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Méthode quasi-Newton: BFGS\n",
    "Pour des problèmes de très grandes tailles, il est parfois très coûteux d'évaluer la hessienne du problème d'optimisation (et même le produit hessienne-vecteur). La famille des méthode *quasi-Newton* construit une approximation $B_k$ symétrique définie positive de la matrice Hessienne en utilisant seulement le gradient et en mesurant sa variation, et permet quand même d'améliorer significativement les performances comparé à la méthode du gradient.\n",
    "$$\n",
    "s_k = x_{k+1} - x_k, \\quad y_k = \\nabla f(x_{k+1}) - \\nabla f(x_k).\n",
    "$$\n",
    "Par ailleurs la matrice $B_k$ est aussi construite de façon à ce que l'inverse soit connue, il n'y a donc pas de système linéaire à résoudre.\n",
    "\n",
    "La méthode la plus connue dans la famille des méthodes quasi-Newton, est la méthode BFGS (Broyden - Fletcher, Goldfarb, and Shanno), et utilise la formule suivante pour calculer l'inverse de $B_k$ que l'on note $H_k$:\n",
    "$$\n",
    "H_{k+1} = (I - \\rho_k s_ky_k^T)H_k(I-\\rho_ky_ks_k^T) + \\rho_ks_ks_k^T, \\quad \\rho_k = \\frac{1}{y_k^Ts_k}.\n",
    "$$\n",
    "L'algorithme est presque le même que la méthode de Newton à la différence qu'il n'y a pas de système linéaire à résoudre et la direction $d_k$ est à coup sûr une direction de descente. Ainsi la direction de descente est calculée comme suit:\n",
    "$$\n",
    "d_k = - H_k \\nabla f(x_k).\n",
    "$$\n",
    "\n",
    "Comment choisir la matrice $H_0$? On peut éventuellement choisir $I$. Une alternative est d'utiliser $H_0=I$ pour la première itération et ensuite mettre $H_0$ à jour avant de calculer $H_1$ en utilisant:\n",
    "$$\n",
    "H_0 = \\frac{y_k^Ts_k}{y_k^Ty_k}I.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Important**: pour s'assurer que la matrice $H_k$ reste définie positive à toutes les itérations, il faut s'assurer que $y_k^Ts_k>0$. C'est toujours vrai pour des fonctions convexes, mais pas nécessairement dans le cas général. On pourra tester ici la version \"skip\" qui ne mets pas à jour quand cette condition n'est pas vérifiée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "bfgs_quasi_newton_armijo (generic function with 1 method)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Solution: copier-coller votre newton_armijo ici et modifier le calcul de la direction avec la méthode de BFGS inverse skip.\n",
    "function bfgs_quasi_newton_armijo(nlp, x0; verbose::Bool = true)\n",
    "  xk = x0\n",
    "  fk  = obj(nlp, xk)\n",
    "  gk = grad(nlp, xk)\n",
    "  gnorm = gnorm0 = norm(gk)  # gnorm0 = norm(gk)\n",
    "  k = 0\n",
    "  t=0\n",
    "  verbose && @printf \"%2s %9s %9s %7s\\n\" \"k\" \"fk\" \"||∇f(x)||\" \"t\"\n",
    "  verbose && @printf \"%2d %9.2e %9.1e %7.1e\\n\" k fk gnorm t\n",
    "\n",
    "  # première itération\n",
    "  H0 = I(length(xk))\n",
    "  d0 = - H0*gk\n",
    "  t = armijo(xk, d0, fk, gk, x -> obj(nlp, x))\n",
    "  xk_1 = xk + t * d0\n",
    "  fk = obj(nlp, xk_1)\n",
    "  gk = grad(nlp, xk_1)\n",
    "  gnorm = norm(gk)\n",
    "  k += 1\n",
    "  verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "\n",
    "  yk = grad(nlp,xk_1) - grad(nlp,xk)\n",
    "  sk = xk_1 - xk \n",
    "  ρk = 1/(transpose(yk)*sk)\n",
    "  Hk = (transpose(yk)*sk)/(transpose(yk)*yk)*I(length(xk_1))\n",
    "  while gnorm > 1.0e-6 + 1.0e-6 * gnorm0 && k < 100 && fk > -1e15\n",
    "    if transpose(yk)*sk > 0 \n",
    "      Hk_1 = (I(length(xk_1)) - ρk*sk*transpose(yk))* Hk * (I(length(xk_1)) - ρk*yk*transpose(sk)) + ρk*sk*transpose(sk)\n",
    "    else \n",
    "      Hk_1 = Hk\n",
    "    end\n",
    "    dk = - Hk_1 *grad(nlp,xk_1)\n",
    "    xk = xk_1\n",
    "    fk = obj(nlp,xk_1)\n",
    "    gk = grad(nlp, xk_1)\n",
    "    t = armijo(xk_1, dk, fk, gk, x -> obj(nlp, x))\n",
    "    xk_1 += t * dk \n",
    "    gnorm = norm(gk)\n",
    "    k += 1\n",
    "    verbose && @printf \"%2d %9.2e %9.1e %7.1e \\n\" k fk gnorm t\n",
    "    yk = grad(nlp,xk_1) - grad(nlp,xk)\n",
    "    sk = xk_1 - xk \n",
    "    ρk = 1/(transpose(yk)*sk)\n",
    "  end\n",
    "  return xk\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||       t\n",
      " 0  0.00e+00   0.0e+00 0.0e+00\n",
      " 1  0.00e+00   0.0e+00 1.0e+00 \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "2-element Vector{Float64}:\n",
       " 0.0\n",
       " 0.0"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Test\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "nlp = ADNLPModel(f, zeros(2))\n",
    "x0 = zeros(2)\n",
    "bfgs_quasi_newton_armijo(nlp, x0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||       t\n",
      " 0  1.74e+05   3.3e+04 0.0e+00\n",
      " 1  2.73e+04   8.6e+03 1.0e-03 \n",
      " 2  2.73e+04   8.6e+03 1.0e+00 \n",
      " 3  1.26e+03   8.6e+02 1.0e+00 \n",
      " 4  7.23e+02   5.8e+02 1.0e+00 \n",
      " 5  1.70e+02   2.1e+02 1.0e+00 \n",
      " 6  5.22e+01   9.6e+01 1.0e+00 \n",
      " 7  1.18e+01   4.1e+01 1.0e+00 \n",
      " 8  2.14e+00   1.8e+01 1.0e+00 \n",
      " 9  3.01e-01   7.7e+00 1.0e+00 \n",
      "10  1.05e-01   4.6e+00 1.0e+00 \n",
      "11  8.39e-02   3.7e+00 1.0e+00 \n",
      "12  7.10e-02   3.0e+00 1.0e+00 \n",
      "13  6.29e-02   2.5e+00 1.0e+00 \n",
      "14  5.72e-02   2.0e+00 1.0e+00 \n",
      "15  5.32e-02   1.8e+00 1.0e+00 \n",
      "16  4.97e-02   1.6e+00 1.0e+00 \n",
      "17  4.66e-02   1.6e+00 1.0e+00 \n",
      "18  4.31e-02   1.7e+00 1.0e+00 \n",
      "19  3.93e-02   1.8e+00 1.0e+00 \n",
      "20  3.47e-02   1.9e+00 1.0e+00 \n",
      "21  2.96e-02   2.0e+00 1.0e+00 \n",
      "22  2.40e-02   2.0e+00 1.0e+00 \n",
      "23  1.89e-02   2.0e+00 1.0e+00 \n",
      "24  1.46e-02   1.7e+00 1.0e+00 \n",
      "25  1.16e-02   1.5e+00 1.0e+00 \n",
      "26  9.41e-03   1.2e+00 1.0e+00 \n",
      "27  8.04e-03   1.0e+00 1.0e+00 \n",
      "28  7.10e-03   8.2e-01 1.0e+00 \n",
      "29  6.47e-03   7.0e-01 1.0e+00 \n",
      "30  5.97e-03   6.1e-01 1.0e+00 \n",
      "31  5.56e-03   5.7e-01 1.0e+00 \n",
      "32  5.16e-03   5.6e-01 1.0e+00 \n",
      "33  4.76e-03   5.8e-01 1.0e+00 \n",
      "34  4.28e-03   6.2e-01 1.0e+00 \n",
      "35  3.74e-03   6.6e-01 1.0e+00 \n",
      "36  3.12e-03   6.9e-01 1.0e+00 \n",
      "37  2.50e-03   6.8e-01 1.0e+00 \n",
      "38  1.93e-03   6.3e-01 1.0e+00 \n",
      "39  1.50e-03   5.6e-01 1.0e+00 \n",
      "40  1.18e-03   4.7e-01 1.0e+00 \n",
      "41  9.75e-04   3.8e-01 1.0e+00 \n",
      "42  8.39e-04   3.1e-01 1.0e+00 \n",
      "43  7.51e-04   2.6e-01 1.0e+00 \n",
      "44  6.87e-04   2.2e-01 1.0e+00 \n",
      "45  6.39e-04   2.0e-01 1.0e+00 \n",
      "46  5.95e-04   1.8e-01 1.0e+00 \n",
      "47  5.54e-04   1.8e-01 1.0e+00 \n",
      "48  5.08e-04   1.9e-01 1.0e+00 \n",
      "49  4.56e-04   2.1e-01 1.0e+00 \n",
      "50  3.94e-04   2.2e-01 1.0e+00 \n",
      "51  3.27e-04   2.3e-01 1.0e+00 \n",
      "52  2.59e-04   2.2e-01 1.0e+00 \n",
      "53  2.01e-04   2.1e-01 1.0e+00 \n",
      "54  1.56e-04   1.8e-01 1.0e+00 \n",
      "55  1.25e-04   1.5e-01 1.0e+00 \n",
      "56  1.04e-04   1.2e-01 1.0e+00 \n",
      "57  9.04e-05   1.0e-01 1.0e+00 \n",
      "58  8.11e-05   8.2e-02 1.0e+00 \n",
      "59  7.47e-05   7.1e-02 1.0e+00 \n",
      "60  6.93e-05   6.4e-02 1.0e+00 \n",
      "61  6.47e-05   6.1e-02 1.0e+00 \n",
      "62  5.99e-05   6.2e-02 1.0e+00 \n",
      "63  5.48e-05   6.5e-02 1.0e+00 \n",
      "64  4.86e-05   7.0e-02 1.0e+00 \n",
      "65  4.18e-05   7.4e-02 1.0e+00 \n",
      "66  3.41e-05   7.6e-02 1.0e+00 \n",
      "67  2.70e-05   7.3e-02 1.0e+00 \n",
      "68  2.07e-05   6.6e-02 1.0e+00 \n",
      "69  1.62e-05   5.7e-02 1.0e+00 \n",
      "70  1.30e-05   4.7e-02 1.0e+00 \n",
      "71  1.10e-05   3.8e-02 1.0e+00 \n",
      "72  9.63e-06   3.1e-02 1.0e+00 \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " k        fk ||∇f(x)||       t\n",
      " 0  0.00e+00   4.5e+00 0.0e+00\n",
      " 1 -3.73e-01   4.2e+00 8.8e-02 \n",
      " 2 -3.73e-01   4.2e+00 1.0e+00 \n",
      " 3 -7.22e-01   1.4e+00 1.0e+00 \n",
      " 4 -8.07e-01   1.1e+00 1.0e+00 \n",
      " 5 -9.46e-01   7.8e-01 1.0e+00 \n",
      " 6 -9.96e-01   2.8e-01 1.0e+00 \n",
      " 7 -1.00e+00   7.2e-02 1.0e+00 \n",
      " 8 -1.00e+00   2.1e-02 1.0e+00 \n",
      " 9 -1.00e+00   9.9e-03 1.0e+00 \n",
      "10 -1.00e+00   5.2e-03 1.0e+00 \n",
      "11 -1.00e+00   4.9e-03 1.0e+00 \n",
      "12 -1.00e+00   4.6e-03 1.0e+00 \n",
      "13 -1.00e+00   1.8e-03 1.0e+00 \n",
      "14 -1.00e+00   4.8e-04 1.0e+00 \n",
      "15 -1.00e+00   2.1e-04 1.0e+00 \n",
      "16 -1.00e+00   1.1e-04 1.0e+00 \n",
      "17 -1.00e+00   9.4e-05 1.0e+00 \n",
      "18 -1.00e+00   9.3e-05 1.0e+00 \n",
      "19 -1.00e+00   4.3e-05 1.0e+00 \n",
      "20 -1.00e+00   1.1e-05 1.0e+00 \n",
      "21 -1.00e+00   4.7e-06 1.0e+00 \n",
      "sol = [0.9999990683502704, -2.6409525493569896e-7]\n",
      " k        fk ||∇f(x)||       t\n",
      " 0  1.00e+00   4.5e+00 0.0e+00\n",
      " 1  3.33e-01   4.4e+00 3.0e-01 \n",
      " 2  3.33e-01   4.4e+00 1.0e+00 \n",
      " 3 -4.00e-01   1.4e+00 1.0e+00 \n",
      " 4 -5.80e-01   1.3e+00 6.7e-01 \n",
      " 5 -8.63e-01   1.8e+00 1.0e+00 \n",
      " 6 -9.72e-01   8.8e-01 1.0e+00 \n",
      " 7 -9.99e-01   1.4e-01 1.0e+00 \n",
      " 8 -1.00e+00   2.2e-02 1.0e+00 \n",
      " 9 -1.00e+00   7.2e-03 1.0e+00 \n",
      "10 -1.00e+00   1.6e-03 6.7e-01 \n",
      "11 -1.00e+00   9.7e-04 1.0e+00 \n",
      "12 -1.00e+00   7.0e-05 1.0e+00 \n",
      "13 -1.00e+00   2.3e-05 1.0e+00 \n",
      "14 -1.00e+00   4.8e-07 1.0e+00 \n",
      "sol = [0.9999999007201239, -2.9564350443627747e-8]\n",
      "\u001b[0m\u001b[1mTest Summary:                         | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "Test set for bfgs_quasi_newton_armijo | \u001b[32m   5  \u001b[39m\u001b[36m    5  \u001b[39m\u001b[0m0.5s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"Test set for bfgs_quasi_newton_armijo\", Any[], 5, false, false, true, 1.675178242864879e9, 1.67517824332894e9)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Tests secrets 3: contactez votre professeur pour avoir ces tests additionels si vous êtes prêt.\n",
    "using Test\n",
    "\n",
    "@testset \"Test set for bfgs_quasi_newton_armijo\" begin\n",
    "\t#Test problem:\n",
    "\tfH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "\tx0H = [10., 20.]\n",
    "\thimmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "\tproblem2 = ADNLPModel(x->-x[1]^3 + x[2]^2 + x[3]^2, ones(3))\n",
    "\n",
    "\troz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "\trosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "\tf(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "\tpb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]\n",
    "\n",
    "\n",
    "\t######################################### bfgs_quasi_newton_armijo ##################\n",
    "\n",
    "\t#Unit/Validation Tests\n",
    "\tusing Logging, Test\n",
    "    ep1 = 1e-6 + norm(grad(himmelblau, himmelblau.meta.x0)) * 1e-6\n",
    "\tsol = bfgs_quasi_newton_armijo(himmelblau, himmelblau.meta.x0) \n",
    "\t@test norm(grad(himmelblau, sol)) ≤ ep1\n",
    "\n",
    "    ep4 = 1e-6 + norm(grad(pb_du_cours, [1.5, .5])) * 1e-6\n",
    "\tsol = bfgs_quasi_newton_armijo(pb_du_cours, [1.5, .5]) \n",
    "\t@test norm(grad(pb_du_cours, sol)) ≤ ep4\n",
    "\t@test sol ≈ [1, 0] atol = ep4\n",
    "\t@show sol\n",
    "\n",
    "    ep5 = 1e-6 + norm(grad(pb_du_cours, [.5, .5])) * 1e-6\n",
    "\tsol = bfgs_quasi_newton_armijo(pb_du_cours, [.5, .5]) \n",
    "\t@test norm(grad(pb_du_cours, sol)) ≤ ep4\n",
    "\t@test sol ≈ [1, 0] atol = ep4\n",
    "\t@show sol\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercice 4: application à un problème de grande taille"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va ajouter le package `OptimizationProblems` qui contient, comme son nom l'indique, une collection de problème d'optimisation disponible au format de `JuMP` (dans le sous-module `OptimizationProblems.PureJuMP`) et de `ADNLPModel` (dans le sous-module `OptimizationProblems.ADNLPProblems`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import Pkg; Pkg.add(\"OptimizationProblems\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "using ADNLPModels, OptimizationProblems.ADNLPProblems # Attention si vous ne faites pas using ADNLPModels avant ça ne fonctionne pas!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 500\n",
    "model = genrose(n=n)\n",
    "@test typeof(model) <: ADNLPModel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Si vous le souhaitez, il est possible d'accéder à certaines informations sur le problème en accédant à son meta:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using OptimizationProblems\n",
    "OptimizationProblems.genrose_meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est aussi possible d'accéder au meta de tous les problèmes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OptimizationProblems.meta"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Résoudre le problème `genrose` et un autre problème de la collection en utilisant vos algorithmes précédents.\n",
    "Avant d'utiliser l'algorithme on testera que le problème est bien sans contrainte avec:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "unconstrained(nlp) #qui retourne vrai si `nlp` est un problème sans contraintes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use previous functions to solve genrose."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.5",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
