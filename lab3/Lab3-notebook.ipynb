{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # MTH8408 : Méthodes d'optimisation et contrôle optimal\n",
    " ## Laboratoire 3: Optimisation sans contraintes et méthodes itératives\n",
    "Tangi Migot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra, NLPModels, Printf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ADNLPModel - Model with automatic differentiation backend ADModelBackend{\n",
       "  ForwardDiffADGradient,\n",
       "  ForwardDiffADHvprod,\n",
       "  ForwardDiffADJprod,\n",
       "  ForwardDiffADJtprod,\n",
       "  SparseADJacobian{Tuple{RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:ˍ₋arg1,), Symbolics.var\"#_RGF_ModTag\", Symbolics.var\"#_RGF_ModTag\", (0x5352fe05, 0x272fa0db, 0x5c038b36, 0x4d71ccf2, 0x675a33ca)}, RuntimeGeneratedFunctions.RuntimeGeneratedFunction{(:ˍ₋out, :ˍ₋arg1), Symbolics.var\"#_RGF_ModTag\", Symbolics.var\"#_RGF_ModTag\", (0x6ac3fb04, 0x162749f8, 0xd9a9f028, 0xfe567ad4, 0x75ba338b)}}},\n",
       "  ForwardDiffADHessian,\n",
       "  ForwardDiffADGHjvprod,\n",
       "}\n",
       "  Problem name: Generic\n",
       "   All variables: ████████████████████ 2      All constraints: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            free: ████████████████████ 2                 free: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                lower: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                upper: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              low/upp: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                fixed: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "          infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               infeas: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "            nnzh: (  0.00% sparsity)   3               linear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                    nonlinear: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "                                                         nnzj: (------% sparsity)         \n",
       "\n",
       "  Counters:\n",
       "             obj: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 grad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 cons: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "        cons_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0             cons_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 jcon: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jgrad: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                  jac: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0              jac_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "         jac_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                jprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0            jprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "       jprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jtprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0           jtprod_lin: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "      jtprod_nln: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                 hess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0                hprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n",
       "           jhess: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0               jhprod: ⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅⋅ 0     \n"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Test problem:\n",
    "using ADNLPModels\n",
    "fH(x) = (x[2]+x[1].^2-11).^2+(x[1]+x[2].^2-7).^2\n",
    "x0H = [10., 20.]\n",
    "himmelblau = ADNLPModel(fH, x0H)\n",
    "\n",
    "problem2 = ADNLPModel(x->-x[1]^2, ones(3))\n",
    "\n",
    "roz(x) = 100 *  (x[2] - x[1]^2)^2 + (x[1] - 1.0)^2\n",
    "rosenbrock = ADNLPModel(roz, [-1.2, 1.0])\n",
    "\n",
    "f(x) = x[1]^2 * (2*x[1] - 3) - 6*x[1]*x[2] * (x[1] - x[2] - 1)\n",
    "pb_du_cours = ADNLPModel(f, [-1.001, -1.001]) #ou [1.5, .5] ou [.5, .5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Commentaires sur Julia\n",
    "\n",
    "Quelques commentaires sur des morceaux de codes que vous avez vu:\n",
    "- les structures, exemple [GenericExecutionStats](https://github.com/JuliaSmoothOptimizers/SolverCore.jl/blob/0091f437a26a27ac8aa53d5e37647223722f7f7c/src/stats.jl#L60) (constructeur, attribut, type).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using SolverCore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mE\u001b[22m\u001b[0m\u001b[1mx\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mc\u001b[22m\u001b[0m\u001b[1mu\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mi\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "GenericExecutionStats(nlp; ...)\n",
       "\\end{verbatim}\n",
       "A GenericExecutionStats is a struct for storing output information of solvers. It contains the following fields:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{status}: Indicates the output of the solver. Use \\texttt{show\\_statuses()} for the full list;\n",
       "\n",
       "\n",
       "\\item \\texttt{solution}: The final approximation returned by the solver (default: an uninitialized vector like \\texttt{nlp.meta.x0});\n",
       "\n",
       "\n",
       "\\item \\texttt{objective}: The objective value at \\texttt{solution} (default: \\texttt{Inf});\n",
       "\n",
       "\n",
       "\\item \\texttt{dual\\_feas}: The dual feasibility norm at \\texttt{solution} (default: \\texttt{Inf});\n",
       "\n",
       "\n",
       "\\item \\texttt{primal\\_feas}: The primal feasibility norm at \\texttt{solution} (default: \\texttt{0.0} if uncontrained, \\texttt{Inf} otherwise);\n",
       "\n",
       "\n",
       "\\item \\texttt{multipliers}: The Lagrange multipliers wrt to the constraints (default: an uninitialized vector like \\texttt{nlp.meta.y0});\n",
       "\n",
       "\n",
       "\\item \\texttt{multipliers\\_L}: The Lagrange multipliers wrt to the lower bounds on the variables (default: an uninitialized vector like \\texttt{nlp.meta.x0} if there are bounds, or a zero-length vector if not);\n",
       "\n",
       "\n",
       "\\item \\texttt{multipliers\\_U}: The Lagrange multipliers wrt to the upper bounds on the variables (default: an uninitialized vector like \\texttt{nlp.meta.x0} if there are bounds, or a zero-length vector if not);\n",
       "\n",
       "\n",
       "\\item \\texttt{iter}: The number of iterations computed by the solver (default: \\texttt{-1});\n",
       "\n",
       "\n",
       "\\item \\texttt{elapsed\\_time}: The elapsed time computed by the solver (default: \\texttt{Inf});\n",
       "\n",
       "\n",
       "\\item \\texttt{solver\\_specific::Dict\\{Symbol,Any\\}}: A solver specific dictionary.\n",
       "\n",
       "\\end{itemize}\n",
       "The constructor preallocates storage for the fields above. Special storage may be used for \\texttt{multipliers\\_L} and \\texttt{multipliers\\_U} by passing them to the constructor. For instance, if a problem has few bound constraints, those multipliers could be held in sparse vectors.\n",
       "\n",
       "The following fields indicate whether the information above has been updated and is reliable:\n",
       "\n",
       "\\begin{itemize}\n",
       "\\item \\texttt{solution\\_reliable}\n",
       "\n",
       "\n",
       "\\item \\texttt{objective\\_reliable}\n",
       "\n",
       "\n",
       "\\item \\texttt{residuals\\_reliable} (for \\texttt{dual\\_feas} and \\texttt{primal\\_feas})\n",
       "\n",
       "\n",
       "\\item \\texttt{multipliers\\_reliable} (for \\texttt{multipliers})\n",
       "\n",
       "\n",
       "\\item \\texttt{bounds\\_multipliers\\_reliable} (for \\texttt{multipliers\\_L} and \\texttt{multipliers\\_U})\n",
       "\n",
       "\n",
       "\\item \\texttt{iter\\_reliable}\n",
       "\n",
       "\n",
       "\\item \\texttt{time\\_reliable}\n",
       "\n",
       "\n",
       "\\item \\texttt{solver\\_specific\\_reliable}.\n",
       "\n",
       "\\end{itemize}\n",
       "Setting fields using one of the methods \\texttt{set\\_solution!()}, \\texttt{set\\_objective!()}, etc., also marks the field value as reliable.\n",
       "\n",
       "The \\texttt{reset!()} method marks all fields as unreliable.\n",
       "\n",
       "\\texttt{nlp} is mandatory to set default optional fields.  All other variables can be input as keyword arguments.\n",
       "\n",
       "Notice that \\texttt{GenericExecutionStats} does not compute anything, it simply stores.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "GenericExecutionStats(nlp; ...)\n",
       "```\n",
       "\n",
       "A GenericExecutionStats is a struct for storing output information of solvers. It contains the following fields:\n",
       "\n",
       "  * `status`: Indicates the output of the solver. Use `show_statuses()` for the full list;\n",
       "  * `solution`: The final approximation returned by the solver (default: an uninitialized vector like `nlp.meta.x0`);\n",
       "  * `objective`: The objective value at `solution` (default: `Inf`);\n",
       "  * `dual_feas`: The dual feasibility norm at `solution` (default: `Inf`);\n",
       "  * `primal_feas`: The primal feasibility norm at `solution` (default: `0.0` if uncontrained, `Inf` otherwise);\n",
       "  * `multipliers`: The Lagrange multipliers wrt to the constraints (default: an uninitialized vector like `nlp.meta.y0`);\n",
       "  * `multipliers_L`: The Lagrange multipliers wrt to the lower bounds on the variables (default: an uninitialized vector like `nlp.meta.x0` if there are bounds, or a zero-length vector if not);\n",
       "  * `multipliers_U`: The Lagrange multipliers wrt to the upper bounds on the variables (default: an uninitialized vector like `nlp.meta.x0` if there are bounds, or a zero-length vector if not);\n",
       "  * `iter`: The number of iterations computed by the solver (default: `-1`);\n",
       "  * `elapsed_time`: The elapsed time computed by the solver (default: `Inf`);\n",
       "  * `solver_specific::Dict{Symbol,Any}`: A solver specific dictionary.\n",
       "\n",
       "The constructor preallocates storage for the fields above. Special storage may be used for `multipliers_L` and `multipliers_U` by passing them to the constructor. For instance, if a problem has few bound constraints, those multipliers could be held in sparse vectors.\n",
       "\n",
       "The following fields indicate whether the information above has been updated and is reliable:\n",
       "\n",
       "  * `solution_reliable`\n",
       "  * `objective_reliable`\n",
       "  * `residuals_reliable` (for `dual_feas` and `primal_feas`)\n",
       "  * `multipliers_reliable` (for `multipliers`)\n",
       "  * `bounds_multipliers_reliable` (for `multipliers_L` and `multipliers_U`)\n",
       "  * `iter_reliable`\n",
       "  * `time_reliable`\n",
       "  * `solver_specific_reliable`.\n",
       "\n",
       "Setting fields using one of the methods `set_solution!()`, `set_objective!()`, etc., also marks the field value as reliable.\n",
       "\n",
       "The `reset!()` method marks all fields as unreliable.\n",
       "\n",
       "`nlp` is mandatory to set default optional fields.  All other variables can be input as keyword arguments.\n",
       "\n",
       "Notice that `GenericExecutionStats` does not compute anything, it simply stores.\n"
      ],
      "text/plain": [
       "\u001b[36m  GenericExecutionStats(nlp; ...)\u001b[39m\n",
       "\n",
       "  A GenericExecutionStats is a struct for storing output information of\n",
       "  solvers. It contains the following fields:\n",
       "\n",
       "    •  \u001b[36mstatus\u001b[39m: Indicates the output of the solver. Use \u001b[36mshow_statuses()\u001b[39m\n",
       "       for the full list;\n",
       "\n",
       "    •  \u001b[36msolution\u001b[39m: The final approximation returned by the solver (default:\n",
       "       an uninitialized vector like \u001b[36mnlp.meta.x0\u001b[39m);\n",
       "\n",
       "    •  \u001b[36mobjective\u001b[39m: The objective value at \u001b[36msolution\u001b[39m (default: \u001b[36mInf\u001b[39m);\n",
       "\n",
       "    •  \u001b[36mdual_feas\u001b[39m: The dual feasibility norm at \u001b[36msolution\u001b[39m (default: \u001b[36mInf\u001b[39m);\n",
       "\n",
       "    •  \u001b[36mprimal_feas\u001b[39m: The primal feasibility norm at \u001b[36msolution\u001b[39m (default: \u001b[36m0.0\u001b[39m\n",
       "       if uncontrained, \u001b[36mInf\u001b[39m otherwise);\n",
       "\n",
       "    •  \u001b[36mmultipliers\u001b[39m: The Lagrange multipliers wrt to the constraints\n",
       "       (default: an uninitialized vector like \u001b[36mnlp.meta.y0\u001b[39m);\n",
       "\n",
       "    •  \u001b[36mmultipliers_L\u001b[39m: The Lagrange multipliers wrt to the lower bounds on\n",
       "       the variables (default: an uninitialized vector like \u001b[36mnlp.meta.x0\u001b[39m\n",
       "       if there are bounds, or a zero-length vector if not);\n",
       "\n",
       "    •  \u001b[36mmultipliers_U\u001b[39m: The Lagrange multipliers wrt to the upper bounds on\n",
       "       the variables (default: an uninitialized vector like \u001b[36mnlp.meta.x0\u001b[39m\n",
       "       if there are bounds, or a zero-length vector if not);\n",
       "\n",
       "    •  \u001b[36miter\u001b[39m: The number of iterations computed by the solver (default:\n",
       "       \u001b[36m-1\u001b[39m);\n",
       "\n",
       "    •  \u001b[36melapsed_time\u001b[39m: The elapsed time computed by the solver (default:\n",
       "       \u001b[36mInf\u001b[39m);\n",
       "\n",
       "    •  \u001b[36msolver_specific::Dict{Symbol,Any}\u001b[39m: A solver specific dictionary.\n",
       "\n",
       "  The constructor preallocates storage for the fields above. Special storage\n",
       "  may be used for \u001b[36mmultipliers_L\u001b[39m and \u001b[36mmultipliers_U\u001b[39m by passing them to the\n",
       "  constructor. For instance, if a problem has few bound constraints, those\n",
       "  multipliers could be held in sparse vectors.\n",
       "\n",
       "  The following fields indicate whether the information above has been updated\n",
       "  and is reliable:\n",
       "\n",
       "    •  \u001b[36msolution_reliable\u001b[39m\n",
       "\n",
       "    •  \u001b[36mobjective_reliable\u001b[39m\n",
       "\n",
       "    •  \u001b[36mresiduals_reliable\u001b[39m (for \u001b[36mdual_feas\u001b[39m and \u001b[36mprimal_feas\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mmultipliers_reliable\u001b[39m (for \u001b[36mmultipliers\u001b[39m)\n",
       "\n",
       "    •  \u001b[36mbounds_multipliers_reliable\u001b[39m (for \u001b[36mmultipliers_L\u001b[39m and \u001b[36mmultipliers_U\u001b[39m)\n",
       "\n",
       "    •  \u001b[36miter_reliable\u001b[39m\n",
       "\n",
       "    •  \u001b[36mtime_reliable\u001b[39m\n",
       "\n",
       "    •  \u001b[36msolver_specific_reliable\u001b[39m.\n",
       "\n",
       "  Setting fields using one of the methods \u001b[36mset_solution!()\u001b[39m, \u001b[36mset_objective!()\u001b[39m,\n",
       "  etc., also marks the field value as reliable.\n",
       "\n",
       "  The \u001b[36mreset!()\u001b[39m method marks all fields as unreliable.\n",
       "\n",
       "  \u001b[36mnlp\u001b[39m is mandatory to set default optional fields. All other variables can be\n",
       "  input as keyword arguments.\n",
       "\n",
       "  Notice that \u001b[36mGenericExecutionStats\u001b[39m does not compute anything, it simply\n",
       "  stores."
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? GenericExecutionStats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Les arguments dans les fonctions. Lire attentivement [la documentation Julia sur les fonctions](https://docs.julialang.org/en/v1/manual/functions/) pour comprendre l'utilisation des `Optional Arguments` et des `Keywords Arguments`. Ce type d'arguments est très utile dans nos applictions où les solveurs dépendent de paramètre dont on peut fixer des valeurs par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 1: Méthode BFGS avec mémoire limitée (L-BFGS)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'implémenter la méthode BFGS à mémoire limitée vue en cours en utilisant les `InverseLBFGSOperator` du package `LinearOperators.jl`. Il y a aussi un petit exemple dans la documentation du package [LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1](https://juliasmoothoptimizers.github.io/LinearOperators.jl/dev/tutorial/#Limited-memory-BFGS-and-SR1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearOperators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "search: \u001b[0m\u001b[1mI\u001b[22m\u001b[0m\u001b[1mn\u001b[22m\u001b[0m\u001b[1mv\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ms\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mL\u001b[22m\u001b[0m\u001b[1mB\u001b[22m\u001b[0m\u001b[1mF\u001b[22m\u001b[0m\u001b[1mG\u001b[22m\u001b[0m\u001b[1mS\u001b[22m\u001b[0m\u001b[1mO\u001b[22m\u001b[0m\u001b[1mp\u001b[22m\u001b[0m\u001b[1me\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\u001b[0m\u001b[1ma\u001b[22m\u001b[0m\u001b[1mt\u001b[22m\u001b[0m\u001b[1mo\u001b[22m\u001b[0m\u001b[1mr\u001b[22m\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "InverseLBFGSOperator(T, n, [mem=5; scaling=true])\n",
       "InverseLBFGSOperator(n, [mem=5; scaling=true])\n",
       "\\end{verbatim}\n",
       "Construct a limited-memory BFGS approximation in inverse form. If the type \\texttt{T} is omitted, then \\texttt{Float64} is used.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "InverseLBFGSOperator(T, n, [mem=5; scaling=true])\n",
       "InverseLBFGSOperator(n, [mem=5; scaling=true])\n",
       "```\n",
       "\n",
       "Construct a limited-memory BFGS approximation in inverse form. If the type `T` is omitted, then `Float64` is used.\n"
      ],
      "text/plain": [
       "\u001b[36m  InverseLBFGSOperator(T, n, [mem=5; scaling=true])\u001b[39m\n",
       "\u001b[36m  InverseLBFGSOperator(n, [mem=5; scaling=true])\u001b[39m\n",
       "\n",
       "  Construct a limited-memory BFGS approximation in inverse form. If the type \u001b[36mT\u001b[39m\n",
       "  is omitted, then \u001b[36mFloat64\u001b[39m is used."
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? InverseLBFGSOperator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ce qui est important dans ce type de méthode est:\n",
    "- le paramètre mémoire\n",
    "- la mise à jour de l'opérateur avec la fonction `push!`\n",
    "- si on a pas une direction de descente, alors on skip\n",
    "- recherche linéaire d'Armijo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "push!(collection, items...) -> collection\n",
       "\\end{verbatim}\n",
       "Insert one or more \\texttt{items} in \\texttt{collection}. If \\texttt{collection} is an ordered container, the items are inserted at the end (in the given order).\n",
       "\n",
       "\\section{Examples}\n",
       "\\begin{verbatim}\n",
       "julia> push!([1, 2, 3], 4, 5, 6)\n",
       "6-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       "\\end{verbatim}\n",
       "If \\texttt{collection} is ordered, use \\href{@ref}{\\texttt{append!}} to add all the elements of another collection to it. The result of the preceding example is equivalent to \\texttt{append!([1, 2, 3], [4, 5, 6])}. For \\texttt{AbstractSet} objects, \\href{@ref}{\\texttt{union!}} can be used instead.\n",
       "\n",
       "See \\href{@ref}{\\texttt{sizehint!}} for notes about the performance model.\n",
       "\n",
       "See also \\href{@ref}{\\texttt{pushfirst!}}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "push!(op, s, y)\n",
       "push!(op, s, y, Bs)\n",
       "push!(op, s, y, α, g)\n",
       "push!(op, s, y, α, g, Bs)\n",
       "\\end{verbatim}\n",
       "Push a new \\{s,y\\} pair into a L-BFGS operator. The second calling sequence is used for forward updating damping, using the preallocated vector \\texttt{Bs}. If the operator is damped, the first call will create \\texttt{Bs} and call the second call. The third and fourth calling sequences are used in inverse LBFGS updating in conjunction with damping, where α is the most recent steplength and g the gradient used when solving \\texttt{d=-Hg}.\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "push!(op, s, y)\n",
       "\\end{verbatim}\n",
       "Push a new \\{s,y\\} pair into a L-SR1 operator.\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "push!(collection, items...) -> collection\n",
       "```\n",
       "\n",
       "Insert one or more `items` in `collection`. If `collection` is an ordered container, the items are inserted at the end (in the given order).\n",
       "\n",
       "# Examples\n",
       "\n",
       "```jldoctest\n",
       "julia> push!([1, 2, 3], 4, 5, 6)\n",
       "6-element Vector{Int64}:\n",
       " 1\n",
       " 2\n",
       " 3\n",
       " 4\n",
       " 5\n",
       " 6\n",
       "```\n",
       "\n",
       "If `collection` is ordered, use [`append!`](@ref) to add all the elements of another collection to it. The result of the preceding example is equivalent to `append!([1, 2, 3], [4, 5, 6])`. For `AbstractSet` objects, [`union!`](@ref) can be used instead.\n",
       "\n",
       "See [`sizehint!`](@ref) for notes about the performance model.\n",
       "\n",
       "See also [`pushfirst!`](@ref).\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "push!(op, s, y)\n",
       "push!(op, s, y, Bs)\n",
       "push!(op, s, y, α, g)\n",
       "push!(op, s, y, α, g, Bs)\n",
       "```\n",
       "\n",
       "Push a new {s,y} pair into a L-BFGS operator. The second calling sequence is used for forward updating damping, using the preallocated vector `Bs`. If the operator is damped, the first call will create `Bs` and call the second call. The third and fourth calling sequences are used in inverse LBFGS updating in conjunction with damping, where α is the most recent steplength and g the gradient used when solving `d=-Hg`.\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "push!(op, s, y)\n",
       "```\n",
       "\n",
       "Push a new {s,y} pair into a L-SR1 operator.\n"
      ],
      "text/plain": [
       "\u001b[36m  push!(collection, items...) -> collection\u001b[39m\n",
       "\n",
       "  Insert one or more \u001b[36mitems\u001b[39m in \u001b[36mcollection\u001b[39m. If \u001b[36mcollection\u001b[39m is an ordered\n",
       "  container, the items are inserted at the end (in the given order).\n",
       "\n",
       "\u001b[1m  Examples\u001b[22m\n",
       "\u001b[1m  ≡≡≡≡≡≡≡≡≡≡\u001b[22m\n",
       "\n",
       "\u001b[36m  julia> push!([1, 2, 3], 4, 5, 6)\u001b[39m\n",
       "\u001b[36m  6-element Vector{Int64}:\u001b[39m\n",
       "\u001b[36m   1\u001b[39m\n",
       "\u001b[36m   2\u001b[39m\n",
       "\u001b[36m   3\u001b[39m\n",
       "\u001b[36m   4\u001b[39m\n",
       "\u001b[36m   5\u001b[39m\n",
       "\u001b[36m   6\u001b[39m\n",
       "\n",
       "  If \u001b[36mcollection\u001b[39m is ordered, use \u001b[36mappend!\u001b[39m to add all the elements of another\n",
       "  collection to it. The result of the preceding example is equivalent to\n",
       "  \u001b[36mappend!([1, 2, 3], [4, 5, 6])\u001b[39m. For \u001b[36mAbstractSet\u001b[39m objects, \u001b[36munion!\u001b[39m can be used\n",
       "  instead.\n",
       "\n",
       "  See \u001b[36msizehint!\u001b[39m for notes about the performance model.\n",
       "\n",
       "  See also \u001b[36mpushfirst!\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  push!(op, s, y)\u001b[39m\n",
       "\u001b[36m  push!(op, s, y, Bs)\u001b[39m\n",
       "\u001b[36m  push!(op, s, y, α, g)\u001b[39m\n",
       "\u001b[36m  push!(op, s, y, α, g, Bs)\u001b[39m\n",
       "\n",
       "  Push a new {s,y} pair into a L-BFGS operator. The second calling sequence is\n",
       "  used for forward updating damping, using the preallocated vector \u001b[36mBs\u001b[39m. If the\n",
       "  operator is damped, the first call will create \u001b[36mBs\u001b[39m and call the second call.\n",
       "  The third and fourth calling sequences are used in inverse LBFGS updating in\n",
       "  conjunction with damping, where α is the most recent steplength and g the\n",
       "  gradient used when solving \u001b[36md=-Hg\u001b[39m.\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  push!(op, s, y)\u001b[39m\n",
       "\n",
       "  Push a new {s,y} pair into a L-SR1 operator."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? LinearOperators.push!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo (generic function with 1 method)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function armijo(xk, dk, fk, gk, slope, nlp :: AbstractNLPModel; τ1 = 1.0e-4, t_update = 1.5)\n",
    "  t = 1.0\n",
    "  fk_new = obj(nlp, xk + dk) # t = 1.0\n",
    "  while fk_new > fk + τ1 * t * slope\n",
    "    t /= t_update\n",
    "    fk_new = obj(nlp, xk + t * dk)\n",
    "  end\n",
    "  return t, fk_new\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "limited_bfgs (generic function with 1 method)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function limited_bfgs(nlp      :: AbstractNLPModel;\n",
    "                      x        :: AbstractVector = nlp.meta.x0,\n",
    "                      atol     :: Real = √eps(eltype(x)), \n",
    "                      rtol     :: Real = √eps(eltype(x)),\n",
    "                      max_eval :: Int = -1,\n",
    "                      max_time :: Float64 = 30.0,\n",
    "                      f_min    :: Float64 = -1.0e16,\n",
    "                      verbose  :: Bool = true,\n",
    "                      mem      :: Int = 5)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  xt = zeros(T, n)\n",
    "  ∇ft = zeros(T, n)\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = InverseLBFGSOperator(n; mem=mem)\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "#################################################\n",
    "    d = - H * ∇f\n",
    "#################################################\n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, ft = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    xt = x + t * d\n",
    "    ∇ft = grad(nlp, xt) # grad!(nlp, xt, ∇ft)\n",
    "#################################################\n",
    "    push!(H, xt - x, ∇ft - ∇f)\n",
    "#################################################\n",
    "\n",
    "    # Move on.\n",
    "    x = xt\n",
    "    f = ft\n",
    "    ∇f = ∇ft\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(\n",
    "        nlp,\n",
    "        status=status,\n",
    "        solution=x,\n",
    "        objective=f,\n",
    "        dual_feas=∇fNorm,\n",
    "        iter=iter,\n",
    "        elapsed_time=elapsed_time,\n",
    "    )\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: InverseLBFGSOperator not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: InverseLBFGSOperator not defined",
      "",
      "Stacktrace:",
      " [1] limited_bfgs(nlp::ADNLPModel{Float64, Vector{Float64}, Vector{Int64}}; x::Vector{Float64}, atol::Float64, rtol::Float64, max_eval::Int64, max_time::Float64, f_min::Float64, verbose::Bool, mem::Int64)",
      "   @ Main .\\In[4]:22",
      " [2] top-level scope",
      "   @ In[5]:3"
     ]
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire\n",
    "limited_bfgs(pb_du_cours; mem=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [3.584428266659278, -1.8481265666485829])\n",
      "(stats.status, stats.solution) = (:unbounded, [1.29140163e8, 1.0, 1.0])\n",
      "(stats.status, stats.solution) = (:first_order, [0.9999999887950609, 0.9999999782159007])\n",
      "(stats.status, stats.solution) = (:unbounded, [-975544.6831042847, -764227.9248199855])\n",
      "(stats.status, stats.solution) = (:first_order, [0.9999999962625671, -3.1168150200102845e-9])\n",
      "(stats.status, stats.solution) = (:first_order, [0.99999999073849, -6.617373493448244e-9])\n",
      "\u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "test set      | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m1.7s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"test set\", Any[], 9, false, false, true, 1.675698520179e9, 1.675698521893e9)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Test\n",
    "# Demander le test secret pour lbfgs\n",
    "@testset begin\n",
    "    #Unit/Validation Tests\n",
    "    using Logging, Test\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(himmelblau) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.solution ≈ [3.584428266659278, -1.8481265666485827] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(problem2) \n",
    "    end\n",
    "    @test stats.status == :unbounded\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(rosenbrock) \n",
    "    end\n",
    "    @test stats.solution ≈ [1., 1.] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(pb_du_cours, x = [-1.001, -1.001]) \n",
    "    end\n",
    "    @test stats.status == :unbounded\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(pb_du_cours, x = [1.5, .5]) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.solution ≈ [1., 0.] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        limited_bfgs(pb_du_cours, x = [.5, .5]) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.solution ≈ [1., 0.] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus:\n",
    "\n",
    "- Compare l'implémentation de `limited_bfgs` avec la fonction `lbfgs` qui est disponible dans `JSOSolvers.jl`.\n",
    "- On veut pouvoir tester \"facilement\" plusieurs valeurs de $\\tau$ et du paramètre de mise à jour dans `armijo` sur les problèmes tests. Comment modifier le code pour que ça soit possible?\n",
    "\n",
    "On peut mesurer deux executions de fonctions Julia grâce aux fonctions de `BenchmarkTools.jl`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "@time expr\n",
       "@time \"description\" expr\n",
       "\\end{verbatim}\n",
       "A macro to execute an expression, printing the time it took to execute, the number of allocations, and the total number of bytes its execution caused to be allocated, before returning the value of the expression. Any time spent garbage collecting (gc), compiling new code, or recompiling invalidated code is shown as a percentage.\n",
       "\n",
       "Optionally provide a description string to print before the time report.\n",
       "\n",
       "In some cases the system will look inside the \\texttt{@time} expression and compile some of the called code before execution of the top-level expression begins. When that happens, some compilation time will not be counted. To include this time you can run \\texttt{@time @eval ...}.\n",
       "\n",
       "See also \\href{@ref}{\\texttt{@showtime}}, \\href{@ref}{\\texttt{@timev}}, \\href{@ref}{\\texttt{@timed}}, \\href{@ref}{\\texttt{@elapsed}}, and \\href{@ref}{\\texttt{@allocated}}.\n",
       "\n",
       "\\begin{quote}\n",
       "\\textbf{note}\n",
       "\n",
       "Note\n",
       "\n",
       "For more serious benchmarking, consider the \\texttt{@btime} macro from the BenchmarkTools.jl package which among other things evaluates the function multiple times in order to reduce noise.\n",
       "\n",
       "\\end{quote}\n",
       "\\begin{quote}\n",
       "\\textbf{compat}\n",
       "\n",
       "Julia 1.8\n",
       "\n",
       "The option to add a description was introduced in Julia 1.8.\n",
       "\n",
       "Recompilation time being shown separately from compilation time was introduced in Julia 1.8\n",
       "\n",
       "\\end{quote}\n",
       "\\begin{verbatim}\n",
       "julia> x = rand(10,10);\n",
       "\n",
       "julia> @time x * x;\n",
       "  0.606588 seconds (2.19 M allocations: 116.555 MiB, 3.75% gc time, 99.94% compilation time)\n",
       "\n",
       "julia> @time x * x;\n",
       "  0.000009 seconds (1 allocation: 896 bytes)\n",
       "\n",
       "julia> @time begin\n",
       "           sleep(0.3)\n",
       "           1+1\n",
       "       end\n",
       "  0.301395 seconds (8 allocations: 336 bytes)\n",
       "2\n",
       "\n",
       "julia> @time \"A one second sleep\" sleep(1)\n",
       "A one second sleep: 1.005750 seconds (5 allocations: 144 bytes)\n",
       "\n",
       "julia> for loop in 1:3\n",
       "            @time loop sleep(1)\n",
       "        end\n",
       "1: 1.006760 seconds (5 allocations: 144 bytes)\n",
       "2: 1.001263 seconds (5 allocations: 144 bytes)\n",
       "3: 1.003676 seconds (5 allocations: 144 bytes)\n",
       "\\end{verbatim}\n"
      ],
      "text/markdown": [
       "```\n",
       "@time expr\n",
       "@time \"description\" expr\n",
       "```\n",
       "\n",
       "A macro to execute an expression, printing the time it took to execute, the number of allocations, and the total number of bytes its execution caused to be allocated, before returning the value of the expression. Any time spent garbage collecting (gc), compiling new code, or recompiling invalidated code is shown as a percentage.\n",
       "\n",
       "Optionally provide a description string to print before the time report.\n",
       "\n",
       "In some cases the system will look inside the `@time` expression and compile some of the called code before execution of the top-level expression begins. When that happens, some compilation time will not be counted. To include this time you can run `@time @eval ...`.\n",
       "\n",
       "See also [`@showtime`](@ref), [`@timev`](@ref), [`@timed`](@ref), [`@elapsed`](@ref), and [`@allocated`](@ref).\n",
       "\n",
       "!!! note\n",
       "    For more serious benchmarking, consider the `@btime` macro from the BenchmarkTools.jl package which among other things evaluates the function multiple times in order to reduce noise.\n",
       "\n",
       "\n",
       "!!! compat \"Julia 1.8\"\n",
       "    The option to add a description was introduced in Julia 1.8.\n",
       "\n",
       "    Recompilation time being shown separately from compilation time was introduced in Julia 1.8\n",
       "\n",
       "\n",
       "```julia-repl\n",
       "julia> x = rand(10,10);\n",
       "\n",
       "julia> @time x * x;\n",
       "  0.606588 seconds (2.19 M allocations: 116.555 MiB, 3.75% gc time, 99.94% compilation time)\n",
       "\n",
       "julia> @time x * x;\n",
       "  0.000009 seconds (1 allocation: 896 bytes)\n",
       "\n",
       "julia> @time begin\n",
       "           sleep(0.3)\n",
       "           1+1\n",
       "       end\n",
       "  0.301395 seconds (8 allocations: 336 bytes)\n",
       "2\n",
       "\n",
       "julia> @time \"A one second sleep\" sleep(1)\n",
       "A one second sleep: 1.005750 seconds (5 allocations: 144 bytes)\n",
       "\n",
       "julia> for loop in 1:3\n",
       "            @time loop sleep(1)\n",
       "        end\n",
       "1: 1.006760 seconds (5 allocations: 144 bytes)\n",
       "2: 1.001263 seconds (5 allocations: 144 bytes)\n",
       "3: 1.003676 seconds (5 allocations: 144 bytes)\n",
       "```\n"
      ],
      "text/plain": [
       "\u001b[36m  @time expr\u001b[39m\n",
       "\u001b[36m  @time \"description\" expr\u001b[39m\n",
       "\n",
       "  A macro to execute an expression, printing the time it took to execute, the\n",
       "  number of allocations, and the total number of bytes its execution caused to\n",
       "  be allocated, before returning the value of the expression. Any time spent\n",
       "  garbage collecting (gc), compiling new code, or recompiling invalidated code\n",
       "  is shown as a percentage.\n",
       "\n",
       "  Optionally provide a description string to print before the time report.\n",
       "\n",
       "  In some cases the system will look inside the \u001b[36m@time\u001b[39m expression and compile\n",
       "  some of the called code before execution of the top-level expression begins.\n",
       "  When that happens, some compilation time will not be counted. To include\n",
       "  this time you can run \u001b[36m@time @eval ...\u001b[39m.\n",
       "\n",
       "  See also \u001b[36m@showtime\u001b[39m, \u001b[36m@timev\u001b[39m, \u001b[36m@timed\u001b[39m, \u001b[36m@elapsed\u001b[39m, and \u001b[36m@allocated\u001b[39m.\n",
       "\n",
       "\u001b[36m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[36m\u001b[1mNote\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  For more serious benchmarking, consider the \u001b[36m@btime\u001b[39m macro from the\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  BenchmarkTools.jl package which among other things evaluates the\n",
       "\u001b[36m\u001b[1m  │\u001b[22m\u001b[39m  function multiple times in order to reduce noise.\n",
       "\n",
       "\u001b[39m\u001b[1m  │ \u001b[22m\u001b[39m\u001b[1mJulia 1.8\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  The option to add a description was introduced in Julia 1.8.\n",
       "\u001b[39m\u001b[1m  │\u001b[22m\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  Recompilation time being shown separately from compilation time\n",
       "\u001b[39m\u001b[1m  │\u001b[22m  was introduced in Julia 1.8\n",
       "\n",
       "\u001b[36m  julia> x = rand(10,10);\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> @time x * x;\u001b[39m\n",
       "\u001b[36m    0.606588 seconds (2.19 M allocations: 116.555 MiB, 3.75% gc time, 99.94% compilation time)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> @time x * x;\u001b[39m\n",
       "\u001b[36m    0.000009 seconds (1 allocation: 896 bytes)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> @time begin\u001b[39m\n",
       "\u001b[36m             sleep(0.3)\u001b[39m\n",
       "\u001b[36m             1+1\u001b[39m\n",
       "\u001b[36m         end\u001b[39m\n",
       "\u001b[36m    0.301395 seconds (8 allocations: 336 bytes)\u001b[39m\n",
       "\u001b[36m  2\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> @time \"A one second sleep\" sleep(1)\u001b[39m\n",
       "\u001b[36m  A one second sleep: 1.005750 seconds (5 allocations: 144 bytes)\u001b[39m\n",
       "\u001b[36m  \u001b[39m\n",
       "\u001b[36m  julia> for loop in 1:3\u001b[39m\n",
       "\u001b[36m              @time loop sleep(1)\u001b[39m\n",
       "\u001b[36m          end\u001b[39m\n",
       "\u001b[36m  1: 1.006760 seconds (5 allocations: 144 bytes)\u001b[39m\n",
       "\u001b[36m  2: 1.001263 seconds (5 allocations: 144 bytes)\u001b[39m\n",
       "\u001b[36m  3: 1.003676 seconds (5 allocations: 144 bytes)\u001b[39m"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? @time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercice 2: NewtonCG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le but de cet exercice est d'adapter les méthodes de Newton de façon à résoudre le système linéaire avec une méthode itérative de type gradient conjugué comme suit ($B_k$ représente la matrice hessienne):"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"LineSearchNewtonCG.png\" width=600 height=600 />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "cg_optim (generic function with 1 method)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function cg_optim(H, ∇f)\n",
    "    #setup the tolerance:\n",
    "    n∇f = norm(∇f)\n",
    "#####################################\n",
    "    ϵk = min(0.5, sqrt(n∇f)) * n∇f\n",
    "####################################\n",
    "    n = length(∇f)\n",
    "    z = zeros(n)\n",
    "    r = ∇f\n",
    "    d = -r\n",
    "    \n",
    "    j = 0\n",
    "    while norm(r) ≥ ϵk && j < 3 * n\n",
    "###############################################\n",
    "        if dot(d, H * d) ≤ 0\n",
    "            if j==0\n",
    "                return p = -∇f\n",
    "            else\n",
    "                return p = z\n",
    "            end\n",
    "        end\n",
    "##############################################\n",
    "        α = dot(r, r) / dot(d, H * d)\n",
    "##############################################        \n",
    "        z += α * d\n",
    "        nrr2 = dot(r, r)\n",
    "        r += α * H * d\n",
    "##############################################\n",
    "        β = dot(r, r) / nrr2\n",
    "##############################################\n",
    "        d = -r + β * d\n",
    "        j += 1\n",
    "    end\n",
    "    return z\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ce qui est important ici est qu'on a pas besoin de stocker/évaluer la matrice hessienne entière mais simplement le produit entre la hessienne et un vecteur. Pour un `NLPModels` on utilise:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "Hv = hprod(nlp, x, v; obj_weight=1.0)\n",
       "\\end{verbatim}\n",
       "Evaluate the product of the objective Hessian at \\texttt{x} with the vector \\texttt{v}, with objective function scaled by \\texttt{obj\\_weight}, where the objective Hessian is\n",
       "\n",
       "$$σ ∇²f(x),$$\n",
       "with \\texttt{σ = obj\\_weight} .\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "Hv = hprod(nlp, x, y, v; obj_weight=1.0)\n",
       "\\end{verbatim}\n",
       "Evaluate the product of the Lagrangian Hessian at \\texttt{(x,y)} with the vector \\texttt{v}, with objective function scaled by \\texttt{obj\\_weight}, where the Lagrangian Hessian is\n",
       "\n",
       "$$∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),$$\n",
       "with \\texttt{σ = obj\\_weight} .\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "Hv = hprod(nlp, x, v; obj_weight=1.0)\n",
       "```\n",
       "\n",
       "Evaluate the product of the objective Hessian at `x` with the vector `v`, with objective function scaled by `obj_weight`, where the objective Hessian is\n",
       "\n",
       "$$\n",
       "σ ∇²f(x),\n",
       "$$\n",
       "\n",
       "with `σ = obj_weight` .\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "Hv = hprod(nlp, x, y, v; obj_weight=1.0)\n",
       "```\n",
       "\n",
       "Evaluate the product of the Lagrangian Hessian at `(x,y)` with the vector `v`, with objective function scaled by `obj_weight`, where the Lagrangian Hessian is\n",
       "\n",
       "$$\n",
       "∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),\n",
       "$$\n",
       "\n",
       "with `σ = obj_weight` .\n"
      ],
      "text/plain": [
       "\u001b[36m  Hv = hprod(nlp, x, v; obj_weight=1.0)\u001b[39m\n",
       "\n",
       "  Evaluate the product of the objective Hessian at \u001b[36mx\u001b[39m with the vector \u001b[36mv\u001b[39m, with\n",
       "  objective function scaled by \u001b[36mobj_weight\u001b[39m, where the objective Hessian is\n",
       "\n",
       "\u001b[35m  σ ∇²f(x),\u001b[39m\n",
       "\n",
       "  with \u001b[36mσ = obj_weight\u001b[39m .\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  Hv = hprod(nlp, x, y, v; obj_weight=1.0)\u001b[39m\n",
       "\n",
       "  Evaluate the product of the Lagrangian Hessian at \u001b[36m(x,y)\u001b[39m with the vector \u001b[36mv\u001b[39m,\n",
       "  with objective function scaled by \u001b[36mobj_weight\u001b[39m, where the Lagrangian Hessian\n",
       "  is\n",
       "\n",
       "\u001b[35m  ∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),\u001b[39m\n",
       "\n",
       "  with \u001b[36mσ = obj_weight\u001b[39m ."
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? NLPModels.hprod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/latex": [
       "\\begin{verbatim}\n",
       "H = hess_op(nlp, x; obj_weight=1.0)\n",
       "\\end{verbatim}\n",
       "Return the objective Hessian at \\texttt{x} with objective function scaled by \\texttt{obj\\_weight} as a linear operator. The resulting object may be used as if it were a matrix, e.g., \\texttt{H * v}. The linear operator H represents\n",
       "\n",
       "$$σ ∇²f(x),$$\n",
       "with \\texttt{σ = obj\\_weight} .\n",
       "\n",
       "\\rule{\\textwidth}{1pt}\n",
       "\\begin{verbatim}\n",
       "H = hess_op(nlp, x, y; obj_weight=1.0)\n",
       "\\end{verbatim}\n",
       "Return the Lagrangian Hessian at \\texttt{(x,y)} with objective function scaled by \\texttt{obj\\_weight} as a linear operator. The resulting object may be used as if it were a matrix, e.g., \\texttt{H * v}. The linear operator H represents\n",
       "\n",
       "$$∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),$$\n",
       "with \\texttt{σ = obj\\_weight} .\n",
       "\n"
      ],
      "text/markdown": [
       "```\n",
       "H = hess_op(nlp, x; obj_weight=1.0)\n",
       "```\n",
       "\n",
       "Return the objective Hessian at `x` with objective function scaled by `obj_weight` as a linear operator. The resulting object may be used as if it were a matrix, e.g., `H * v`. The linear operator H represents\n",
       "\n",
       "$$\n",
       "σ ∇²f(x),\n",
       "$$\n",
       "\n",
       "with `σ = obj_weight` .\n",
       "\n",
       "---\n",
       "\n",
       "```\n",
       "H = hess_op(nlp, x, y; obj_weight=1.0)\n",
       "```\n",
       "\n",
       "Return the Lagrangian Hessian at `(x,y)` with objective function scaled by `obj_weight` as a linear operator. The resulting object may be used as if it were a matrix, e.g., `H * v`. The linear operator H represents\n",
       "\n",
       "$$\n",
       "∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),\n",
       "$$\n",
       "\n",
       "with `σ = obj_weight` .\n"
      ],
      "text/plain": [
       "\u001b[36m  H = hess_op(nlp, x; obj_weight=1.0)\u001b[39m\n",
       "\n",
       "  Return the objective Hessian at \u001b[36mx\u001b[39m with objective function scaled by\n",
       "  \u001b[36mobj_weight\u001b[39m as a linear operator. The resulting object may be used as if it\n",
       "  were a matrix, e.g., \u001b[36mH * v\u001b[39m. The linear operator H represents\n",
       "\n",
       "\u001b[35m  σ ∇²f(x),\u001b[39m\n",
       "\n",
       "  with \u001b[36mσ = obj_weight\u001b[39m .\n",
       "\n",
       "  ────────────────────────────────────────────────────────────────────────────\n",
       "\n",
       "\u001b[36m  H = hess_op(nlp, x, y; obj_weight=1.0)\u001b[39m\n",
       "\n",
       "  Return the Lagrangian Hessian at \u001b[36m(x,y)\u001b[39m with objective function scaled by\n",
       "  \u001b[36mobj_weight\u001b[39m as a linear operator. The resulting object may be used as if it\n",
       "  were a matrix, e.g., \u001b[36mH * v\u001b[39m. The linear operator H represents\n",
       "\n",
       "\u001b[35m  ∇²L(x,y) = σ ∇²f(x) + \\sum_i yᵢ ∇²cᵢ(x),\u001b[39m\n",
       "\n",
       "  with \u001b[36mσ = obj_weight\u001b[39m ."
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "? NLPModels.hess_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "armijo_Newton_cg (generic function with 1 method)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "function armijo_Newton_cg(nlp      :: AbstractNLPModel;\n",
    "                          x        :: AbstractVector = nlp.meta.x0,\n",
    "                          atol     :: Real = √eps(eltype(x)), \n",
    "                          rtol     :: Real = √eps(eltype(x)),\n",
    "                          max_eval :: Int = -1,\n",
    "                          max_time :: Float64 = 30.0,\n",
    "                          f_min    :: Float64 = -1.0e16)\n",
    "  start_time = time()\n",
    "  elapsed_time = 0.0\n",
    "\n",
    "  T = eltype(x)\n",
    "  n = nlp.meta.nvar\n",
    "\n",
    "  f = obj(nlp, x)\n",
    "  ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "  H = hess_op(nlp, x)\n",
    "#################################################\n",
    "\n",
    "  ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "  ϵ = atol + rtol * ∇fNorm\n",
    "  iter = 0\n",
    "\n",
    "  @info log_header([:iter, :f, :dual, :slope, :bk], [Int, T, T, T, T],\n",
    "                   hdr_override=Dict(:f=>\"f(x)\", :dual=>\"‖∇f‖\", :slope=>\"∇fᵀd\"))\n",
    "\n",
    "  optimal = ∇fNorm ≤ ϵ\n",
    "  unbdd = f ≤ f_min\n",
    "  tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  stalled = false\n",
    "  status = :unknown\n",
    "\n",
    "  while !(optimal || tired || stalled || unbdd)\n",
    "        \n",
    "    d = cg_optim(H, ∇f)\n",
    "        \n",
    "    slope = dot(d, ∇f)\n",
    "    if slope ≥ 0\n",
    "      @error \"not a descent direction\" slope\n",
    "      status = :not_desc\n",
    "      stalled = true\n",
    "      continue\n",
    "    end\n",
    "\n",
    "    # Perform improved Armijo linesearch.\n",
    "    t, f = armijo(x, d, f, ∇f, slope, nlp)\n",
    "        \n",
    "    @info log_row(Any[iter, f, ∇fNorm, slope, t])\n",
    "\n",
    "    # Update L-BFGS approximation.\n",
    "    x += t * d\n",
    "    ∇f = grad(nlp, x)\n",
    "#################################################\n",
    "    H = hess_op(nlp, x)\n",
    "#################################################\n",
    "\n",
    "    ∇fNorm = norm(∇f) #nrm2(n, ∇f)\n",
    "    iter = iter + 1\n",
    "\n",
    "    optimal = ∇fNorm ≤ ϵ\n",
    "    unbdd = f ≤ f_min\n",
    "    elapsed_time = time() - start_time\n",
    "    tired = neval_obj(nlp) > max_eval ≥ 0 || elapsed_time > max_time\n",
    "  end\n",
    "  @info log_row(Any[iter, f, ∇fNorm])\n",
    "\n",
    "  if optimal\n",
    "    status = :first_order\n",
    "  elseif tired\n",
    "    if neval_obj(nlp) > max_eval ≥ 0\n",
    "      status = :max_eval\n",
    "    elseif elapsed_time > max_time\n",
    "      status = :max_time\n",
    "    end\n",
    "  elseif unbdd\n",
    "        status = :unbounded\n",
    "  end\n",
    "\n",
    "  return GenericExecutionStats(nlp, status = status, solution=x, objective=f, dual_feas=∇fNorm,\n",
    "                               iter=iter, elapsed_time=elapsed_time)\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m  iter      f(x)      ‖∇f‖      ∇fᵀd        bk  \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     0   3.8e+04   3.3e+04  -2.3e+05   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     1   8.0e+03   1.0e+04  -4.9e+04   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     2   1.4e+03   3.1e+03  -1.1e+04   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     3   2.0e+02   9.0e+02  -2.1e+03   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     4   1.9e+01   2.4e+02  -3.1e+02   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     5   2.0e+00   5.6e+01  -3.1e+01   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     6   1.8e-01   1.2e+01  -3.3e+00   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     7   4.2e-02   4.0e+00  -2.8e-01   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     8   5.9e-05   1.8e+00  -8.1e-02   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     9   1.5e-10   6.0e-02  -1.2e-04   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m    10   1.5e-10   9.6e-05\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: first-order stationary\""
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Unit/Validation Tests\n",
    "# Réaliser un test unitaire\n",
    "\n",
    "armijo_Newton_cg(himmelblau)\n",
    "# armijo_Newton_cg(problem2)\n",
    "# armijo_Newton_cg(rosenbrock)\n",
    "# armijo_Newton_cg(pb_du_cours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(stats.status, stats.solution) = (:first_order, [2.9999993048920612, 2.000003199723837])\n",
      "(stats.status, stats.solution) = (:unbounded, [1.29140163e8, 1.0, 1.0])\n",
      "(stats.status, stats.solution) = (:first_order, [0.999999476502548, 0.9999989509130072])\n",
      "(stats.status, stats.solution) = (:unbounded, [-475732.37111010315, -259818.976413054])\n",
      "(stats.status, stats.solution) = (:first_order, [1.000000000013107, 6.553559582523889e-12])\n",
      "(stats.status, stats.solution) = (:first_order, [1.0000000000000009, 4.489396480695633e-16])\n",
      "\u001b[0m\u001b[1mTest Summary: | \u001b[22m\u001b[32m\u001b[1mPass  \u001b[22m\u001b[39m\u001b[36m\u001b[1mTotal  \u001b[22m\u001b[39m\u001b[0m\u001b[1mTime\u001b[22m\n",
      "test set      | \u001b[32m   9  \u001b[39m\u001b[36m    9  \u001b[39m\u001b[0m0.2s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Test.DefaultTestSet(\"test set\", Any[], 9, false, false, true, 1.675699904413e9, 1.67569990458e9)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using Test\n",
    "\n",
    "# Demander le test secret pour newton-cg\n",
    "@testset begin\n",
    "    #Unit/Validation Tests\n",
    "    using Logging, Test\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(himmelblau) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.dual_feas ≤ 1e-6 + 1e-6 * norm(grad(himmelblau, himmelblau.meta.x0))\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(problem2) \n",
    "    end\n",
    "    @test stats.status == :unbounded\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(rosenbrock) \n",
    "    end\n",
    "    @test stats.solution ≈ [1., 1.] atol = 1e-5\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(pb_du_cours, x = [-1.001, -1.001]) \n",
    "    end\n",
    "    @test stats.status == :unbounded\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(pb_du_cours, x = [1.5, .5]) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.solution ≈ [1., 0.] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "    stats = with_logger(NullLogger()) do \n",
    "        armijo_Newton_cg(pb_du_cours, x = [.5, .5]) \n",
    "    end\n",
    "    @test stats.status == :first_order\n",
    "    @test stats.solution ≈ [1., 0.] atol = 1e-6\n",
    "    @show (stats.status, stats.solution)\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Comment préparer un benchmark\n",
    "\n",
    "On veut maintenant pouvoir réaliser un benchmark de plusieurs solveurs. Pour comparer les algorithmes, il nous faut une collection de problèmes tests et on va utiliser `OptimizationProblems.jl`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling OptimizationProblems [5049e819-d29b-5fba-b941-0eee7e64c1c6]\n",
      "WARNING: method definition for #DataFrame#195 at C:\\Users\\Goglu\\.julia\\packages\\DataFrames\\JZ7x5\\src\\dataframe\\dataframe.jl:398 declares type variable T but does not use it.\n",
      "WARNING: method definition for col_ordering at C:\\Users\\Goglu\\.julia\\packages\\DataFrames\\JZ7x5\\src\\abstractdataframe\\sort.jl:145 declares type variable O but does not use it.\n",
      "WARNING: method definition for #camshape#95 at C:\\Users\\Goglu\\.julia\\packages\\OptimizationProblems\\kFBID\\src\\PureJuMP\\camshape.jl:13 declares type variable T but does not use it.\n",
      "WARNING: method definition for #linsv#629 at C:\\Users\\Goglu\\.julia\\packages\\OptimizationProblems\\kFBID\\src\\PureJuMP\\linsv.jl:3 declares type variable T but does not use it.\n",
      "WARNING: method definition for #mgh01feas#635 at C:\\Users\\Goglu\\.julia\\packages\\OptimizationProblems\\kFBID\\src\\PureJuMP\\mgh01feas.jl:3 declares type variable T but does not use it.\n",
      "WARNING: method definition for #triangle#799 at C:\\Users\\Goglu\\.julia\\packages\\OptimizationProblems\\kFBID\\src\\PureJuMP\\triangle.jl:11 declares type variable T but does not use it.\n"
     ]
    }
   ],
   "source": [
    "using OptimizationProblems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous pouvez trouver un tutoriel de comment réaliser un benchmark avec ce package sur la documentation [OptimizationProblems.jl/dev/benchmark/](https://juliasmoothoptimizers.github.io/OptimizationProblems.jl/dev/benchmark/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il est fort possible que les petits problèmes tests que l'on résout après l'implémentation ne suffisent pas à déceler des bugs. Mais on peut toujours analyser l'éxecution de notre algorithme sur certains problèmes de la collection afin d'améliorer la valeur de certains paramètres (limite de temps, d'itérations, d'évaluations), détecter un bug, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: using PureJuMP.rosenbrock in module Main conflicts with an existing identifier.\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39mPrecompiling NLPModelsJuMP [792afdf1-32c1-5681-94e0-d7bf7a5df49e]\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m  iter      f(x)      ‖∇f‖      ∇fᵀd        bk  \n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     0   8.4e-01   3.9e+00  -1.6e+01   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     1   2.0e-01   2.4e+00  -1.4e+01   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     2  -8.4e-01   9.9e-01  -4.2e+00   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     3  -1.4e+00   2.4e+00  -7.4e+00   1.3e-01\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     4  -1.6e+00   1.7e+00  -2.8e-01   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     5  -1.7e+00   2.2e-01  -4.2e-03   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     6  -1.7e+00   2.1e-03  -3.9e-07   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     7  -1.7e+00   1.3e-05  -1.5e-11   1.0e+00\n",
      "\u001b[36m\u001b[1m[ \u001b[22m\u001b[39m\u001b[36m\u001b[1mInfo: \u001b[22m\u001b[39m     8  -1.7e+00   7.0e-10\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"Execution stats: first-order stationary\""
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "using OptimizationProblems.PureJuMP, NLPModelsJuMP\n",
    "jump_model = AMPGO02() # OptimizationProblems.PureJuMP.AMPGO02\n",
    "prbl = MathOptNLPModel(jump_model)\n",
    "limited_bfgs(prbl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vous vous en doutez pour le rapport de cette semaine on va vouloir réaliser une benchmark avec les deux méthodes que l'on a codé."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Appendix:\n",
    "\n",
    "Une petite remarque sur la gestion de la mémoire:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = 1\n",
      "b = 1\n",
      "b = 1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour les nombres:\n",
    "a = 1\n",
    "@show a\n",
    "b = a\n",
    "@show b\n",
    "a = 2\n",
    "@show b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a = [0.0, 0.0]\n",
      "(a, b) = ([0.0, 0.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [0.0, 0.0])\n",
      "(a, b) = ([1.0, 1.0], [1.0, 1.0])\n",
      "(a, b) = ([2.0, 2.0], [2.0, 2.0])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([2.0, 2.0], [2.0, 2.0])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Pour les tableaux:\n",
    "a = zeros(Float64, 2) #or zeros(2)\n",
    "@show a\n",
    "b = a\n",
    "@show (a,b)\n",
    "a = ones(Float64, 2)\n",
    "@show (a,b)\n",
    "\n",
    "#Pour les tableaux:\n",
    "a = ones(Float64, 2)\n",
    "b = a\n",
    "@show (a,b)\n",
    "a .= 2*ones(Float64, 2) #same would go with grad!\n",
    "@show (a,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "ename": "LoadError",
     "evalue": "UndefVarError: nlp not defined",
     "output_type": "error",
     "traceback": [
      "UndefVarError: nlp not defined",
      "",
      "Stacktrace:",
      " [1] top-level scope",
      "   @ In[79]:2"
     ]
    }
   ],
   "source": [
    "#Pour les NLPModels, il existe aussi des fonctions qui interviennent sur la mémoire\n",
    "gk = grad(nlp, x0)\n",
    "grad!(nlp, x0, gk) #équivaut à gk .= grad(nlp, x0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
