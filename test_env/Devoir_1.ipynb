{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Devoir 1, MTH8408"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 1\n",
    "L'algorithme demandé est présenté dans la cellule ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function steepest_descent_exact_linesearch_quadratique_convexe(A, b, x0; eps::Float64 = 1e-6)\n",
    "    # This method solves the quadratic convex function f(x) = 1/2 * (x' A x) + b x\n",
    "    # Using the steepest descent with exact linesearch method\n",
    "\n",
    "    xk = x0                             # Initialize xk\n",
    "    gk = A * xk + b                     # Initialize gradient\n",
    "    gnorm = gnorm0 = gk' * gk           # Initialize norm of gradient and save gnorm0\n",
    "    k = 0\n",
    "    continue_eval = true\n",
    "    while continue_eval\n",
    "        dk = -gk                        # Set descent direction to steepest descent (-gradient)\n",
    "        t = gnorm / (dk' * A * dk)      # Minimize f(xk + t*dk) for t > 0\n",
    "\n",
    "        xk += t * dk                    # Update xk for next iteration\n",
    "        gk = A * xk + b                 # Calculate gradient for next iteration\n",
    "        gnorm = gk' * gk                # Calculate norm of gradient for next iteration\n",
    "        k += 1                          # Update k\n",
    "\n",
    "        # Check stop condition\n",
    "        if gnorm <= eps + eps * gnorm0\n",
    "            # Solution found\n",
    "            continue_eval = false\n",
    "        end\n",
    "    end\n",
    "    return xk\n",
    "  end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2\n",
    "Soit la fonction suivante.\n",
    "$$f(x_1,x_2)=x_1^2x_2^2-4x_1^2x_2+4x_1^2+2x_1x_2^2+x_2^2-8x_1x_2+8x_1-4x_2$$\n",
    "\n",
    "Les points stationnaires $(x_1^*,x_2^*)$ de cette fonctions sont tels que le gradient $\\nabla f(x_1^*,x_2^*)$ est null. \\\n",
    "Par conséquent, on cherche à satisfaire les deux équations suivantes :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\nabla f(x_1^*,x_2^*) = & \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\end{bmatrix} = 0 \\\\\n",
    "    \\nabla f(x_1^*,x_2^*) = & \\begin{bmatrix} 2(x_1^*+1)(x_2^*-2)^2 \\\\ 2(x_1^*+1)^2(x_2^*-2) \\end{bmatrix} = \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On remarque que $x_1=-1$ annule le gradient $\\forall x_2 \\in R$ et $x_2=2$ annule le gradient $\\forall x_1 \\in R$. \\\n",
    "Cela signifie que tous les points $(x_1^*,x_2^*)$ se trouvant sur les droites $x_1=-1$ et $x_2=2$ sont des points stationnaires.\n",
    "\n",
    "On détermine la nature de ces points à partir de la matrice Hessienne. \\\n",
    "On remarque d'ailleurs que la matrice Hessienne est semi-définie positive pour tous les points stationnaires de la fonction.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    H(x_1,x_2) = & \n",
    "    \\begin{bmatrix}\n",
    "        \\frac{\\partial^2 f}{\\partial x_1^2} & \\frac{\\partial^2 f}{\\partial x_1 \\partial x_2} \\\\\n",
    "        \\frac{\\partial^2 f}{\\partial x_2 \\partial x_1} & \\frac{\\partial^2 f}{\\partial x_2^2}\n",
    "    \\end{bmatrix} \\\\\n",
    "    H(x_1,x_2) = & \n",
    "    \\begin{bmatrix}\n",
    "        2(x_2-2)^2 & 4(x_1+1)(x_2-2) \\\\\n",
    "        4(x_1+1)(x_2-2) & 2(x_1+1)^2\n",
    "    \\end{bmatrix} \\\\\n",
    "    H(x_1^*,x_2^*) = &\n",
    "    \\begin{bmatrix}\n",
    "        \\lambda_1 & 0 \\\\\n",
    "        0 & \\lambda_2\n",
    "    \\end{bmatrix}\n",
    "    \\text{ avec $\\lambda_1,\\lambda_2 \\geq 0$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Finalement, on observe que $f(x_1^*,x_2^*)=0$ pour tous les points d'optimalité décrits ci-haut. \\\n",
    "Comme $f(x_1,x_2)=(x_1+1)^2(x_2-2)^2 \\geq 0$, on en déduit que tous ces points stationnaires sont des minimums de la fonction $f$. \\\n",
    "De plus, ceux-ci sont tous des minimums globaux, puisque $f(x_1,x_2) \\geq f(x_1^*,x_2^*) \\, \\forall (x_1,x_2) \\in R^2$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3.\n",
    "### a) Trouver $a_k$ tel que $B_k = B_{k-1} + a_k s_k^T$\n",
    "À partir de l'équation sécante et de la définition de la mise à jour de la matrice $B_k$, on isole le vecteur $a_k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    B_k s_k = & \\,\\, y_k \\\\\n",
    "    (B_{k-1} + a_k s_k^T) s_k = & \\,\\, y_k \\\\\n",
    "    B_{k-1} s_k + a_k s_k^T s_k = & \\,\\, y_k \\\\\n",
    "    a_k = & \\,\\, \\frac{y_k - B_{k-1} s_k}{s_k^T s_k}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Cette méthode n'est pas particulièrement utilisée en optimisation.\n",
    "\n",
    "\n",
    "### b) Trouver $a_k$ tel que $B_k = B_{k-1} + \\sigma_k a_k a_k^T$, avec $\\sigma_k \\in \\{-1, 1\\}$\n",
    "À partir de l'équation sécante et de la définition de la mise à jour de la matrice $B_k$, on isole le vecteur $a_k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    B_k s_k = & \\,\\, y_k \\\\\n",
    "    (B_{k-1} + \\sigma_k a_k a_k^T) s_k = & \\,\\, y_k \\\\\n",
    "    B_{k-1} s_k + \\sigma_k a_k a_k^T s_k = & \\,\\, y_k \\\\\n",
    "    a_k a_k^T s_k = & \\,\\, \\frac{y_k - B_{k-1} s_k}{\\sigma_k} \\\\\n",
    "    a_k = & \\,\\, \\frac{y_k - B_{k-1} s_k}{\\sqrt{\\sigma_k (y_k - B_{k-1} s_k)^T s_k}} \\text{, avec $\\sigma_k$ choisi de sorte à obtenir un dénominateur réel}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Afin d'éviter que le dénominateur ne tende vers $0$ et cause des problèmes de convergences, la condition suivante est utilisée :\n",
    "$$|s_k^T(y_k - B_{k-1} s_k)| \\geq r \\|s_k\\| \\|y_k - B_{k-1} s_k\\| \\text{, avec $r$ très petit, $r \\approx 10^{-8}$}$$\n",
    "\n",
    "Comme avantage, cette méthode garantit la conservation de la symmétrie de la matrice mise à jour $B_k$. \\\n",
    "Par contre, un désavantage est que $B_k$ n'est pas garantie d'être définie positive, il faut choisir $\\sigma_k$ adéquatement afin que ce soit le cas."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4\n",
    "### a) Résoudre le problème à une dimension suivant\n",
    "\n",
    "$$\n",
    "\\max_{t \\in R} t \\| g_k \\|_2^2 - \\frac{1}{2} t^2 L_g \\| g_k \\|_2^2\n",
    "$$\n",
    "\n",
    "On résout le problème en prenant la dérivée par rapport à la variable $t$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{d}{d t} (t \\| g_k \\|_2^2 - \\frac{1}{2} t^2 L_g \\| g_k \\|_2^2) = & \\,\\, 0 \\\\\n",
    "    \\| g_k \\|_2^2 - t L_g \\| g_k \\|_2^2 = & \\,\\, 0 \\\\\n",
    "    t = & \\,\\, \\frac{1}{L_g} \\text{, si $\\| g_k \\|_2^2 \\neq 0$} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On vérifie qu'il s'agit bien d'un maximum avec la dérivée seconde.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\frac{d^2}{d t^2} ( t \\| g_k \\|_2^2 - \\frac{1}{2} t^2 L_g \\| g_k \\|_2^2 ) < & \\,\\, 0 \\\\\n",
    "    - L_g \\| g_k \\|_2^2 < & \\,\\, 0 \\text{, vrai $\\forall t \\in R$} \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Donc $t = \\frac{1}{L_g}$ est un maximum. Par conséquent, la valeur maximale la fonction est :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\max_{t \\in R} t \\| g_k \\|_2^2 - \\frac{1}{2} t^2 L_g \\| g_k \\|_2^2 = & \\,\\, \\frac{1}{L_g} \\| g_k \\|_2^2 - \\frac{1}{2} \\frac{1}{L_g^2} L_g \\| g_k \\|_2^2 \\\\\n",
    "    \\max_{t \\in R} t \\| g_k \\|_2^2 - \\frac{1}{2} t^2 L_g \\| g_k \\|_2^2 = & \\,\\, \\frac{\\| g_k \\|_2^2}{2 L_g}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### b) Démonstration 1.\n",
    "\n",
    "Soit la propriété suivante\n",
    "\n",
    "$$\n",
    "\\| R(x,y) \\|_2 \\leq \\frac{L_g}{2} \\| y-x \\|_2^2\n",
    "$$\n",
    "\n",
    "On montre le résultat demandé à partir du développement de Taylor d'ordre 1 de la fonction $f$ autour du point $x$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(y) = & \\,\\, f(x) + (y-x)^T \\nabla f(x) + R(x,y) \\\\\n",
    "    f(x-t g_k) - f(x) = & \\,\\, -t g_k^T g_k + R(x,x-t g_k) \\\\\n",
    "    f(x-t g_k) - f(x) \\leq & \\,\\, -t \\|g_k\\|_2^2 + \\frac{L_g}{2} \\|x-t g_k-x\\|_2^2 \\\\\n",
    "    f(x) - f(x-t g_k) \\geq & \\,\\, t \\|g_k\\|_2^2 - \\frac{1}{2} t^2 L_g \\|g_k\\|_2^2 \\text{, $\\forall t \\geq 0$}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### c) Démonstration 2.\n",
    "\n",
    "Comme on trouve $x_{k+1} = x_k - t g_k$ par une recherche linéaire exacte, on a le résultat suivant concernant $g_{k+1}$\n",
    "\n",
    "$$\n",
    "g_{k+1}^T g_k = 0\n",
    "$$\n",
    "\n",
    "Par conséquent, on peut réécrire l'hypothèse H2 ainsi :\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\|g_k-g_{k+1}\\|_2 \\leq & \\,\\, L_g \\|x_k - x_{k+1}\\|_2 \\\\\n",
    "    \\|g_k-g_{k+1}\\|_2^2 \\leq & \\,\\, L_g^2 \\|x_k - x_{k+1}\\|_2^2 \\\\\n",
    "    \\|g_k\\|_2^2+\\|g_{k+1}\\|_2^2 - 2 g_{k+1}^T g_k \\leq & \\,\\, L_g^2 \\|t g_k\\|_2 \\\\\n",
    "    \\|g_k\\|_2^2+\\|g_{k+1}\\|_2^2 \\leq & \\,\\, (t L_g)^2 \\|g_k\\|_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On isole ensuite $t$ pour obtenir une borne minimale sur la valeur de $t$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    t \\geq & \\,\\, \\frac{1}{L_g} \\sqrt{1+\\frac{\\|g_{k+1}\\|_2^2}{\\|g_k\\|_2^2}} \\\\\n",
    "    t \\geq & \\,\\, \\frac{1}{L_g}\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Ceci implique que la solution à $\\argmin_{t \\geq 0} f(x_k - t g_k)$ est donnée par $t \\geq \\frac{1}{L_g}$, donc \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_k-t g_k) \\leq & \\,\\, f(x_k-\\frac{1}{L_g} g_k) \\\\\n",
    "    f(x_k) - f(x_k-t g_k) \\geq & \\,\\, f(x_k) - f(x_k-\\frac{1}{L_g} g_k)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On utilise ensuite le résultat de la question précédente pour obtenir $f(x_k) - f(x_k-\\frac{1}{L_g} g_k) \\geq \\frac{1}{2 L_g} \\|g_k\\|_2^2$, donc\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_k) - f(x_k-t g_k) \\geq & \\,\\, f(x_k) - f(x_k-\\frac{1}{L_g} g_k) \\geq \\frac{1}{2 L_g} \\|g_k\\|_2^2 \\\\\n",
    "    f(x_k) - f(x_k-t g_k) \\geq & \\,\\, \\frac{1}{2 L_g} \\|g_k\\|_2^2 \n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "\n",
    "### d) Preuve du nombre maximal d'itérations nécessaires pour atteindre $\\|g_k\\|_2^2 \\leq \\epsilon$\n",
    "\n",
    "Soit un point de départ $x_0$, $f(x_0)$ et $g_0$, le gradient évalué à ce point. On a\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_0) - f(x_0 - t_0 g_0) \\geq & \\,\\, \\frac{1}{2 L_g} \\|g_0\\|_2^2 \\\\\n",
    "    f(x_1) \\leq & \\,\\, f(x_0) - \\frac{1}{2 L_g} \\|g_0\\|_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "Et de même pour tous $k$.\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_k) \\leq & \\,\\, f(x_{k-1}) - \\frac{1}{2 L_g} \\|g_{k-1}\\|_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "En partant de $x_0$ et allant jusqu'à l'itération $k^*$ (identifiant l'obtention d'une solution suffisante), on écrit\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_{k^*}) \\leq & \\,\\, f(x_0) - \\frac{1}{2 L_g} \\sum_{n=0}^{k^*-1} \\|g_{n}\\|_2^2\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "On sait que $\\|g_{n}\\|_2^2 > \\epsilon^2 \\,\\, \\forall n < k^*$, puisque si $\\|g_{n}\\|_2^2 \\leq \\epsilon^2$, alors l'algorithme prend fin et $n=k^*$. \\\n",
    "Posons $\\|g_{n}\\|_2^2 = \\epsilon^2 + \\delta_n$, avec $\\delta_n > 0$ $\\forall n < k^*$. Avec $\\delta_n = \\delta \\rightarrow 0 \\,\\, \\forall n$, on obtient la progression la plus lente possible. Ainsi,\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    f(x_{k^*}) \\leq & \\,\\, \\lim_{\\delta \\rightarrow 0} (f(x_0) - \\frac{1}{2 L_g} \\sum_{n=0}^{k^*-1} (\\epsilon^2 + \\delta)) \\\\\n",
    "    f(x_{k^*}) \\leq & \\,\\, f(x_0) - \\frac{1}{2 L_g} (\\epsilon^2 + \\lim_{\\delta \\rightarrow 0} \\delta) \\sum_{n=0}^{k^*-1} 1 \\\\\n",
    "    f(x_{k^*}) \\leq & \\,\\, f(x_0) - \\frac{1}{2 L_g} \\epsilon^2 k^* \\\\\n",
    "    k^* \\leq & \\,\\, \\frac{2 L_g}{\\epsilon^2}  (f(x_0) - f(x_{k^*}))\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "En supposant qu'il existe au moins un point $x^*$ situé près de $x_{k^*}$ pour lequel $\\nabla f(x^*)=0$ et connaissant une borne minimale pour l'évaluation de la fonction à ce point, dénotée $f(x^*)$ et telle que $f(x^*) \\leq f(x_{k^*})$, on écrit\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    k^* \\leq & \\,\\, \\frac{2 L_g}{\\epsilon^2}  (f(x_0) - f(x_{k^*})) \\leq \\frac{2 L_g}{\\epsilon^2}  (f(x_0) - f(x^*)) \\\\\n",
    "    k^* \\leq & \\,\\, \\frac{2 L_g (f(x_0) - f(x^*))}{\\epsilon^2}\n",
    "\\end{align*}\n",
    "$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.8.4",
   "language": "julia",
   "name": "julia-1.8"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.8.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
